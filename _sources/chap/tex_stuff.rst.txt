Abstract
========

The abstract gives a concise overview of the work you have done. The
reader shall be able to decide whether the work which has been done is
interesting for him by reading the abstract. Provide a brief account on
the following questions:

-  What is the problem you worked on? (Introduction)

-  How did you tackle the problem? (Materials and Methods)

-  What were your results and findings? (Results)

-  Why are your findings significant? (Conclusion)

The abstract should approximately cover half of a page, and does
generally not contain citations.

Acknowledgements
================

The First Appendix
==================

In the appendix, list the following material:

-  Data (evaluation tables, graphs etc.)

-  Program code

-  Further material

Extended Derivations
====================

.. _app:forward:

Forward Process Closed-Form
---------------------------

Starting with transition distributions

.. math:: q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \bm{x}_{t-1}, \beta_t I)

the reparameterization :math:`\alpha = 1 - \beta` is introduced

.. math:: q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \bm{x}_{t-1}, (1-\alpha) I)

which can also be formulated as

.. math:: q(\bm{x}_t|\bm{x}_{t-1}) = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t}\mathcal{N}(\bm{0}, \bm{I}).

For coherent indexing it is beneficial to switch to notation using
random variables

.. math::

   \bm{x}_{t} = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t}\bm{\epsilon_{t-1}}
       \label{eq:forward_randomvar}

where :math:`\bm{\epsilon_{t-1}} \sim \mathcal{N}(\bm{0}, \bm{I})` and
the earlier :math:`\bm{x}_t` can be recursively inserted into the
formula. Recalling that the sum :math:`Z = X + Y` of two normally
distributed random variables
:math:`X \sim \mathcal{N}(\mu_X, \sigma_Y^2)` and
:math:`Y \sim \mathcal{N}(\mu_Y, \sigma_Y^2)` is again normally
distributed according to
:math:`Z \sim \mathcal{N}(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)`

.. math::

   \begin{aligned}
       x_t & = \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1-\alpha_{t-1}}\bm{\epsilon}_{t-2} \right) + \sqrt{1-\alpha_{t}} \bm{\epsilon}_{t-1} \\
           & = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1})} \bm{\epsilon}_{t-2} + \sqrt{1-\alpha_{t}} \bm{\epsilon}_{t-1}         \\
           & = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1}) + (1-\alpha_{t})} \bm{\bar{\epsilon}}_{t-2}\end{aligned}

where :math:`\bm{\bar{\epsilon}}_{t-2}` is the sum of the random
variables up to :math:`t-2` (again Gaussian). The second term can of
course be simplified to

.. math:: \bm{x}_t = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}} \bm{\bar{\epsilon}}_{t-2}

which is exactly the same form as in
Eq. `[eq:forward_randomvar] <#eq:forward_randomvar>`__. Therefore the
final form is

.. math:: \bm{x}_t = \sqrt{\bar{\alpha}_{t}} \bm{x}_{t-2} + \sqrt{1-\bar{\alpha}_{t}} \bm{\bar{\epsilon}}_{t-2}

with :math:`\bar{\alpha_t} = \prod_{s=1}^{t}\alpha_s` as before.

Conclusion
==========

List the conclusions of your work and give evidence for these. Often,
the discussion and the conclusion sections are fused.

Discussion
==========

The discussion section gives an interpretation of what you have done
:raw-latex:`\cite{day2006wap}`:

-  *What do your results mean?* Here you discuss, but you do not
   recapitulate results. Describe principles, relationships and
   generalizations shown. Also, mention inconsistencies or exceptions
   you found.

-  *How do your results relate to other’s work?* Show how your work
   agrees or disagrees with other’s work. Here you can rely on the
   information you presented in the “related work” section.

-  *What are implications and applications of your work?* State how your
   methods may be applied and what implications might be.

Make sure that introduction/related work and the discussion section act
as a pair, i.e. “be sure the discussion section answers what the
introduction section asked” :raw-latex:`\cite{day2006wap}`.

Experiments and Results
=======================

Describe the evaluation you did in a way, such that an independent
researcher can repeat it. Cover the following questions:

-  *What is the experimental setup and methodology?* Describe the
   setting of the experiments and give all the parameters in detail
   which you have used. Give a detailed account of how the experiment
   was conducted.

-  *What are your results?* In this section, a *clear description* of
   the results is given. If you produced lots of data, include only
   representative data here and put all results into the appendix.

.. _sec:forward_diff_experiments:

Influence of Schedules and Image Size on the Forward Diffusion
--------------------------------------------------------------

Ho et al. had derived a closed form solution to the forward process of
DDPMs and Nichol et al. investigated alternative options for the noise
scheduling. :raw-latex:`\autocite{ho2020denoising,nichol2021improved}`
They concluded that the important parameters to model are not the
variances :math:`\beta` of the transitions, but the variances
:math:`1-\bar{\alpha}` of the closed-form forward process, since they
are the ones responsible for the destruction of information.

They decided to go with a squared cosine function, since this would be
close to linear smooth out towards the critical beginning and end points
of the process. In Fig.\ `5.1 <#fig:alphadash>`__ you can see how
:math:`1-\bar{\alpha}` and :math:`\beta` behave for both approaches. It
is immediately visible that the variances reach the maximum too early
and flatten out for the linear schedule. This leads to the intuition
that the last few steps are not very useful.

.. figure:: images/variance_schedule_alphadash.png
   :alt: Variance Schedule Approaches: Modeling the
   :math:`1-\bar{\alpha}` as an approximate linear function (right
   cosine) and deriving :math:`\beta` (left cosine), or modeling
   :math:`\beta` as a linear function (left linear) and deriving
   :math:`1-\bar{\alpha}`.
   :name: fig:alphadash

   Variance Schedule Approaches: Modeling the :math:`1-\bar{\alpha}` as
   an approximate linear function (right cosine) and deriving
   :math:`\beta` (left cosine), or modeling :math:`\beta` as a linear
   function (left linear) and deriving :math:`1-\bar{\alpha}`.

The intution can experimentally confirmed by measuring how closely we
get to isotropic noise when passing samples through the forward process.
For this a batch of 50 times the same image was passed through the
different steps of the process and the covariance matrix was calculated.
As a metric for how close the covariance matrix was to the identity
covariance matrix of pure i.i.d Gaussian noise, the identity matrix was
subtracted and the mean of the absolute value of the matrix calculated.
The results can be seen in Fig. `5.2 <#fig:noisecloseness>`__ and
confirm the intuition: When using linear scheduling we reach the closest
point to pure noise already after around 600 steps for small images, and
after around 700 for larger images. Cosine scheduling also performs
worse on smaller images than on larger ones, but is still capable
providing value for at least 850 timesteps.

.. figure:: images/frobenius_norm.png
   :alt: Closeness to noise for linear scheduling (left) and cosine
   scheduling (right).
   :name: fig:noisecloseness

   Closeness to noise for linear scheduling (left) and cosine scheduling
   (right).

Introduction
============

Give an introduction to the topic you have worked on:

-  *What is the rationale for your work?* Give a sufficient description
   of the problem, e.g. with a general description of the problem
   setting, narrowing down to the particular problem you have been
   working on in your thesis. Allow the reader to understand the problem
   setting.

-  *What is the scope of your work?* Given the above background, state
   briefly the focus of the work, what and how you did.

-  *How is your thesis organized?* It helps the reader to pick the
   interesting points by providing a small text or graph which outlines
   the organization of the thesis. The structure given in this document
   shows how the general structuring shall look like. However, you may
   fuse chapters or change their names according to the requirements of
   your thesis.

Focus of this Work
------------------

Thesis Organization
-------------------

Materials and Methods
=====================

The objectives of the “Materials and Methods” section are the following:

-  *What are tools and methods you used?* Introduce the environment, in
   which your work has taken place - this can be a software package, a
   device or a system description. Make sure sufficiently detailed
   descriptions of the algorithms and concepts (e.g. math) you used
   shall be placed here.

-  *What is your work?* Describe (perhaps in a separate chapter) the key
   component of your work, e.g. an algorithm or software framework you
   have developed.

Latent Variable Models
----------------------

Before getting started it is important to define the terms used in the
next sections, since they all stem from Bayesian statistics. The
Bayesian theorem can be written as

.. math:: p(z|x) = \frac{p(x|z)p(z)}{p(x)}

where it is implicitly assumed that :math:`p` is a probability density
function over two continuous random variables :math:`x` and :math:`z`.
The formula holds in general, but in generative modeling and machine
learning it is usually assumed that the letter :math:`z` represents a
random variable in a latent (unobserved) space from which – after
successful training – new data can be generated by sampling. This
requires that :math:`p(z)` is a simple distribution from which sampling
is easy and that the trained model is capable of mapping values from the
latent distribution to the true data distribution.

Using above described ordering, the four terms in this formula use
distinct names:

:math:`p(x)`
   is called the *evidence* or the *marginal likelihood*. It encompasses
   the actual observations of the data.

:math:`p(z)`
   is called the *prior*, since it exposes information on :math:`z`
   before any conditioning.

:math:`p(z|x)`
   is called the *posterior*. It describes the distribution over
   :math:`z` after (*post*) having seen the evidence :math:`x`.

:math:`p(x|z)`
   is called the *likelihood*. It gives the literal likelihood of
   observing an example :math:`x` when choosing the latent space to be a
   specific :math:`z`.

Variational Autoencoders
------------------------

One of the most straightforward examples of a generative model, where
the goal is to find such a latent space representation of the training
sample distribution, is the Variational Autoencoder
(VAE) :raw-latex:`\autocite{kingma2022autoencoding}`. The name of the
VAE stems from the Autoencoder, a network that tries to recreate its
output through a bottleneck and thereby learns a compressed
representation of the
data. :raw-latex:`\autocite{https://doi.org/10.1002/aic.690370209}`
Autoencoders bear similarity to other dimension reduction methods like
Principal Component Analysis (PCA) and therefore were first published
under the name *Nonlinear principal component analysis*. The
*variational* part in the VAE stems from the fact that it does not only
learn to recreate input samples through dimensionality reduction, but is
also optimized to represent the distribution over the training samples
as a combination of a parameterized latent distribution
:math:`p_{\theta_z}(z)` and a neural network mapping
:math:`p_{\theta_{NN}}(x|z)` between the latent space and the sample
space, termed decoder. The latent distribution is chosen such that
sampling from it is easy (e.g. a multivariate Gaussian, with the
parameters being vectors of means and variances). With sufficient
dimensionality reduction the encoding should not overfit and the latent
space should be a good approximation of the true data manifold. This
enables the creation of data, by sampling the latent space and mapping
it to the output.

Marginalizing :math:`p_\theta(x)` requires another approximation of the
posterior :math:`p(z|x)` with a neural network, termed the encoder
:math:`p_{\theta_{NN_{in}}}(z|x)`.

.. math:: p_{\theta}(x) = \int p_{\theta_{NN}}(x|z) p_{\theta_z}(z) dz = \frac{p_{\theta_{NN_{out}}}(x|z) p_{\theta_z}(z)}{p_{\theta_{NN_{in}}}(z|x)}

A schematic of a VAE, separated into these 3 factors – encoder, latent
distribution and decoder – is shown in Fig. `7.1 <#fig:vae>`__.

.. figure:: images/vae.png
   :alt: VAE schematic: :math:`p(x)` is approximated through a latent
   variable model where posterior and likelihood are modeled with neural
   networks and the prior on the latent variable is modeled through a
   simple parameterized distribution (often Gaussian). The hope is, that
   after training, sampling from :math:`p(z)` and passing it through the
   neural network :math:`p_{\theta_{NN}}(x|z)`, is the same as sampling
   from :math:`p(x)`.
   :name: fig:vae

   VAE schematic: :math:`p(x)` is approximated through a latent variable
   model where posterior and likelihood are modeled with neural networks
   and the prior on the latent variable is modeled through a simple
   parameterized distribution (often Gaussian). The hope is, that after
   training, sampling from :math:`p(z)` and passing it through the
   neural network :math:`p_{\theta_{NN}}(x|z)`, is the same as sampling
   from :math:`p(x)`.

KL Divergence and Variational Lower Bound
-----------------------------------------

In VAEs, the encoder :math:`p_{\theta}(z|x)` needs to approximate the
posterior :math:`p(z|x)`, therefore a differentiable loss function is
needed that compares two probability distributions. One such heavily
used measure is the KL (Kullback-Leibler) divergence

.. math:: KL\left[p_{\theta}(z|x) || p(z|x)\right] = \int \log \frac{p_{\theta}(z|x)}{p(z|x)} p_{\theta}(z|x) dz

which has the properties of being strictly non-negative and is only 0 if
the two distributions are equal.

At the same time, the output of the VAE should fit the true data
distribution well, e.g. should maximize the log-likelihood of the
evidence :math:`p_{\theta}(x|z)`. This term is mainly responsible for
the reconstruction of truthful samples from the latent space. Combining
the two terms and inverting the signs (for minimization rather than
maximization) gives a loss function known as ELBO (evidence lower bound)
or VLB (variational lower bound).

.. math::

   \label{eq:elbo}
       \mathcal{L}_{VLB} = - \log p_{\theta}(x|z) + KL\left[p_{\theta}(z|x) || p(z|x)\right]

Diffusion Denoising Probabilistic Models
----------------------------------------

Diffusion Denoising Probabilistic Models (DDPMs or Diffusion Models) are
a generative model that learn the distribution of images in a training
set. During training, sample images are gradually destroyed by adding
noise over many iterations and a neural network is trained, such that
these steps can be inverted.

As the name suggests, image content is diffused in timesteps, therefore
we use the random variable :math:`\bm{x}_0` to represent our original
training images, :math:`\bm{x}_t` for (partially noisy) images at an
intermediate timestep and :math:`\bm{x}_T` for images at the end of the
process where all information has been destroyed and the distribution
:math:`q(\bm{x}_T)` largely follows an isotropic Gaussian distribution.

The goal is to train a network that creates a less noisy image
:math:`\bm{x}_{t-1}` from :math:`\bm{x}_t`. If this is achieved we
should be able to sample some new :math:`\bm{x}_T` and generate new
samples from the training distribution :math:`q(\bm{x}_0)` by passing
this noisy image many times through the network until the noise is fully
removed.

Forward Diffusion Process
~~~~~~~~~~~~~~~~~~~~~~~~~

Mathematical Description
^^^^^^^^^^^^^^^^^^^^^^^^

In order to derive a training objective it is important to understand
the workings of the *forward diffusion process*. During this process,
i.i.d (independent and identically distributed) Gaussian noise is
applied to the image over many discrete timesteps. A *variance schedule*
defines the means and variances (:math:`\sqrt{1-\beta}` and
:math:`\beta`) of the added noise at every
timestep. :raw-latex:`\autocite{ho2020denoising}` The whole process can
be expressed as a Markov chain (depicted in
Fig. `7.2 <#fig:forward_diffusion>`__), with the factorization

.. math::

   \label{eq:forwardprocess}
       q(\bm{x}_T|\bm{x}_0) = q(\bm{x}_0) \prod_{t=1}^{T} q(\bm{x}_{t}|\bm{x}_{t-1})

where the transition distributions
:math:`q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \bm{x}_{t-1}, \beta_t I)`.
An example of iterative destruction of an image by this process is shown
in Fig. `7.3 <#fig:forward_naoshima>`__.

.. figure:: images/forward_diffusion.png
   :alt: Forward Diffusion Process: An image is iteratively destroyed by
   adding normally distributed noise, according to a schedule. This
   represents a Markov process with the transition probability
   :math:`q(\bm{x}_t|\bm{x}_{t-1})`.
   :name: fig:forward_diffusion

   Forward Diffusion Process: An image is iteratively destroyed by
   adding normally distributed noise, according to a schedule. This
   represents a Markov process with the transition probability
   :math:`q(\bm{x}_t|\bm{x}_{t-1})`.

.. figure:: images/forward_naoshima.png
   :alt: Example of Iterative Image Destruction through Forward
   Diffusion Process: The indices give the time step in the iterative
   destruction process, where :math:`\beta` was created according to a
   linear noise variance schedule (5000 steps from in the 0.001 to 0.02
   range and picture resolution of 4016 by 6016 pixels).
   :name: fig:forward_naoshima

   Example of Iterative Image Destruction through Forward Diffusion
   Process: The indices give the time step in the iterative destruction
   process, where :math:`\beta` was created according to a linear noise
   variance schedule (5000 steps from in the 0.001 to 0.02 range and
   picture resolution of 4016 by 6016 pixels).

Gladly it is not necessary to sample noise again and again in order to
arrive at :math:`\bm{x}_t`, since Ho et al. derived a closed-form
solution to the sampling
procedure. :raw-latex:`\autocite{ho2020denoising}` For this, the
variance schedule is first reparameterized as :math:`1-\beta = \alpha`

.. math::

   q(\bm{x}_t | \bm{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \bm{x}_{t-1}, (1-\alpha_t) \bm{I})
       \label{eq:forward_alpha}

and the closed-form solution for :math:`q(\bm{x}_t|\bm{x}_0)` is derived
by introducing the cumulative product
:math:`\bar{\alpha_t} = \prod_{s=1}^{t}\alpha_s` as

.. math::

   q(\bm{x}_t|\bm{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha_t}}\bm{x}_0, (1-\bar{\alpha_t})\bm{I})
       \label{eq:forward_alphadash}

A choice of :math:`\bar{\alpha_t} \in [0,1]` in above parameterizaiton
ensures that the variance does not explode in the process, but that the
SNR (signal-to-noise-ratio) still goes to 0 by gradually attenuating the
means, corresponding to the original image. Thanks to the
reparameterization with :math:`\bar{\alpha_t}`, the forward process is
also not restricted anymore to discrete timesteps, but a continuous
schedule can be
used. :raw-latex:`\autocite{kingma2023variational,song2021scorebased}`

The derivation that leads from
Eq. `[eq:forward_alpha] <#eq:forward_alpha>`__ to
Eq. `[eq:forward_alphadash] <#eq:forward_alphadash>`__ is left to
appendix `2.1 <#app:forward>`__.

Influence of Scheduling Functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The process of information destruction is dependent on the chosen
variance schedule, the number of steps and the image size. Beyond the
most simple case – a constant variance over time – Ho et al. opted for
the second most simple option, a linear schedule, where the variance
:math:`\beta_t` grows linearly in
:math:`t`. :raw-latex:`\autocite{ho2020denoising}` Nichol et al. later
found that a cosine-based schedule gives better results on lower
resolution images, since it does not destruct information quite as
quickly, making it more informative in the last few timesteps. They also
mention that their cosine schedule is purely based on intuition and they
similar functions to perform equally
well. :raw-latex:`\autocite{nichol2021improved}` Own experiments
exploring above mentioned parameters are explained
in `5.1 <#sec:forward_diff_experiments>`__ and plots of the two
different variance schedules are visible in
Fig. `5.1 <#fig:alphadash>`__.

Reverse Diffusion Process
~~~~~~~~~~~~~~~~~~~~~~~~~

DDPMs can be viewed as latent space models in a similar way that
Generative Adversarial Nets or Variational Autoencoders
can. :raw-latex:`\autocite{goodfellow2014generative,kingma2022autoencoding}`

In DDPMs the reverse process is again a Markov chain and can therefore
again be factorized as

.. math::

   \label{eq:reverseprocess}
       q(\bm{x}_0|\bm{x}_T) = q(\bm{x}_T) \prod_{t=T}^{1} q(\bm{x}_{t-1}|\bm{x}_{t})

which means that our network does not learn to approximate the full
inversion, but rather just the transition probabilities
:math:`q(\bm{x}_{t-1}|\bm{x}_{t})` in the chain, which are transitions
between several intermediate latent distributions. Sohl-Dickstein et al.
further showed that the reverse transitions are also Gaussian in the
limit of :math:`t \rightarrow 0`, e.g. as long as the diffusion steps
are small enough. We therefore approximate

.. math::

   \label{eq:reverseapprox}
       q(\bm{x}_{t-1} | \bm{x}_t) \approx p_{\theta}(\bm{x}_{t-1} | \bm{x}_t) = \mathcal{N}(\bm{\mu}_{\theta}(\bm{x}_t, t),\bm{\Sigma}_{\theta}(\bm{x}_t, t)).

Loss Functions
~~~~~~~~~~~~~~

The combination of forward :math:`q(\bm{x}_T|\bm{x}_0)` and reverse
process :math:`q(\bm{x}_0|\bm{x}_T)` can be viewed as a chain of many
VAEs and we can again formulate a variational lower bound objective
(Eq. `[eq:elbo] <#eq:elbo>`__) that maximizes log-likelihood of the
output and matches transition
probabilities. :raw-latex:`\autocite{nichol2021improved}`

.. math::

   \begin{aligned}
       \mathcal{L}_0       & = - \log p_{\theta}(x_0|x_1)                                    \\
       \mathcal{L}_{1:T-1} & = KL\left[q(x_{t-1}|x_t, x_0) || p_{\theta}(x_{t-1}|x_t)\right] \\
       \mathcal{L}_T       & = KL\left[q(x_T|x_0) || p(x_T)\right]\end{aligned}

The exact posterior :math:`q(x_{t-1}|x_t, x_0)` is tractable for
specific samples of the training distribution, therefore
:math:`\mathcal{L}_{1:T-1}` could be calculated, since KL divergence has
a closed form solution for two Gaussian distributions.
:math:`\mathcal{L}_T` is independent of :math:`\theta` and therefore not
used for optimization. It should anyway be very close to zero if the
parameterization of the forward process is correct and forward diffused
samples get close to :math:`\mathcal{N}(0,\bm{I})`. The first term is
only used for performance evaluation in terms of log-likelihood, but not
in optimization.

Another simplification is usually taken and
:math:`p_{\theta}(\bm{x}_{t-1} | \bm{x}_t)` only approximates the means
:math:`\bm{\mu}_{\theta}` and not the variances. For small enough
timesteps, the means determine the transitional distributions much
stronger than the variances. The network is furhter usually trained to
not predict the means directly, but the noise and the means are then
determined through a
reparameterization. :raw-latex:`\autocite{ho2020denoising,nichol2021improved}`

Image Guided Diffusion
----------------------

Both, Choi et al. and Lugmayr et al. make use of unconditional DDPMs for
image-guided diffusion for the tasks of image translation in the former
and in-painting in the
latter. :raw-latex:`\autocite{choi2021ilvr,lugmayr2022repaint}`
Similarly, classifier guidance or CLIP-guidance can be used on
unconditional and conditional DDPMs to produce samples of a specific
class or matching a prompt in the unconditional case or to further trade
off sample variability for sample
fidelity. :raw-latex:`\autocite{dhariwal2021diffusion}` We show here
that both approaches can be interpreted as special cases of MAP (maximum
a posteriori) estimation and that they can be generalized to other means
of guidance during the reverse process.

MAP Estimation for Inverse Problems
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MAP estimation is a statistical concept that is often used for
optimizing the parameters :math:`\theta` of a parameterized distribution
to observed data :math:`z\sim p(z)` following Bayes rule

.. math:: \hat{\theta}_{MAP} = \argmax_{\theta} p(\theta | z) = \argmax_{\theta}\frac{p(z|\theta)p(\theta)}{p(z)}

or for inverse problems

.. math:: \hat{x}_{MAP} = \argmax_x p(x|s) = \argmax_x \frac{p(s|x)p(x)}{p(s)}

where :math:`s \sim p(s)` is the evidence that is provided by the
measured signal, :math:`p(x)` is a prior on the desired reconstruction
and the likelihood term :math:`p(s|x)` enforces a data consistency
between the measured signal and the true distribution, usually a forward
model of the data-corrupting process. Since maximizing :math:`p(x|s)` is
the same as maximizing :math:`\log(x|s)` and :math:`p(s)` is independent
of :math:`\theta`, we can separate the product into a sum.

.. math::

   \begin{aligned}
       \hat{x}_{MAP} = \argmax_x log p(x|s) & = \argmax_x \log{\frac{p(s|x)p(x)}{p(s)}} \\
                                            & = \argmax_x \log p(s|x)p(x)               \\
                                            & = \argmax_x \log f(x, s)\end{aligned}

Such problems are usually optimized using iterative optimization schemes
such as gradient ascent
:math:`x_{t+1} = x_{t} + \lambda \nabla_{x} f(x, s)`, with
:math:`\lambda` being the step length. It is usually helpful to separate
the data consistency term (likelihood) and regularizer (prior) into
individual terms for joint optimization with their respective gradients
and weights.

.. math::

   \begin{aligned}
       \label{eq:mapestimation}
       x_{i+1} = x_{i} + \lambda_1 \nabla_{x_i} \log p(s|x_i) + \lambda_2 \nabla_{x_i} \log p(x_i)\end{aligned}

DDPMs as Priors
~~~~~~~~~~~~~~~

DDPMs approximate a data distribution over training images :math:`p(x)`
and according to Song et al., they do so by learning to approximate
gradients of the distribution and taking a gradient ascent step with
every iteration of the reverse diffusion
process. :raw-latex:`\autocite{song2020generative}` With this
interpretation the DDPM has the exact same form as
Eq. `[eq:mapestimation] <#eq:mapestimation>`__ without the data
consistency term :math:`p(s|x)`.

.. math::

   \label{eq:ddpmiteration}
       x_{t-1} = x_{t} + \nabla_{x_t} \log p(x_t)

Classifier Guidance
~~~~~~~~~~~~~~~~~~~

Classifier guidance as termed by Nichol et al. introduces a data
consistency term :math:`p(s|x_t)` in the form of a classifier trained on
noisy images, where :math:`s` is the random variable expressing if an
image belongs to a certain
class. :raw-latex:`\autocite{dhariwal2021diffusion,sohldickstein2015deep}`
Conditioning on a classifier is sucessfully used by taking gradient
ascent steps not only in the direction that maximizes the prior
:math:`p(x)` in a DDPM :math:`\nabla_{x_t} \log p(x_t)`, but also the
direction of this conditioning term :math:`\nabla_{x_t} \log p(s|x_t)`.
In total, this is equal to
Eq. `[eq:mapestimation] <#eq:mapestimation>`__

.. math:: x_{t+1} = \underbrace{x_{t} + \nabla_{x_t} \log p(x_t)}_{x'_{t+1}} + \lambda \nabla_{x_t} \log p(s|x_t)

with :math:`x'_{t+1}` being the prediction of the reverse diffusion
steps before any conditioning and :math:`\lambda` an arbitrary factor
determining the strength of the guidance.

Gradients and Closed-Forms of Data Consistency Terms
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Starting from Eq. `[eq:ddpmiteration] <#eq:ddpmiteration>`__, the data
consistency term can be reintroduced into the DDPM model.

Choi et al. guide the diffusion process by trying to match the low
frequency content of a target image to the low frequency content of the
prediction :math:`\argmin_x \phi(s) - phi(x)`. They do this by using a
very simple linear data consistency function, corresponding to a
difference between linear low-pass filtered representations, which they
call ILVR (iterative latent variable refinement)

.. math::

   \begin{aligned}
       \label{eq:ilvr}
       x_{t-1} & = \phi(s_{t}) + (I - \phi) (x_{t})  \\
       x_{t-1} & = x_{t} + \phi(s_{t}) - \phi(x_{t}) \\
       x_{t-1} & = x_{t} + \phi(s_{t} -x_{t})\end{aligned}

where :math:`\phi` is a linear filter operation and :math:`s_t` is
obtained by using the forward process on the target
image. :raw-latex:`\autocite{choi2021ilvr}`

Similarly, Lugmayr et al. use a conditioning on known parts of the image
for the inpainting operation, which effectively boils down to applying a
linear mask that zeroes out known parts in the prediction and replaces
them with outputs from the forward process.

.. math::

   \begin{aligned}
       \label{eq:repaint}
       % Mx_t (0 at unknown, 1 at known) removes known parts from x_t and leaves unknown
       % Ms_t (0 at unknown, 1 at known) leaves prediction in unknown region and replaces known region
       x_{t-1} & = x_t - \mathcal{M}(x_t) + \mathcal{M}(s_t) \\
       x_{t-1} & = x_t + \mathcal{M}(s_t - x_t)\end{aligned}

As is easily seen, Eq. `[eq:ilvr] <#eq:ilvr>`__ and
Eq. `[eq:repaint] <#eq:repaint>`__ take the same form and can be
interpretated as simultaneously taking gradient ascent steps for
optimizing :math:`\argmax_x \log p(x)` and
:math:`\argmax_x \log p(s|x)`.

Assuming distribution over latent predictions and targets at diffusion
timestep :math:`t`, :math:`p(s|t)` and :math:`p(x|t)`, and
:math:`\mathcal{L}` corresponding to an arbitrary linear operation

.. math::

   \begin{aligned}
       \mathcal{L}(p(s|t) - p(x|t))                                                       & = \nabla_{x_t} \log p(s_t|x_t)         \\
       \int \mathcal{L}(p(s|t) - p(x|t)) dt                                               & = \int \nabla_{x_t} \log p(s_t|x_t) dt \\
       \int \mathcal{L}(p(s|t))dt - \int \mathcal{L}(p(x|t))dt                            & = \log p(s|x)                          \\
       \mathcal{L} \left(\int p(s|t)dt \right) - \mathcal{L} \left( \int p(x|t)dt \right) & = \log p(s|x)                          \\
       \mathcal{L}(p(s)) - \mathcal{L}(p(x))                                              & = \log p(s|x)\end{aligned}

where the left side of the equation is the original

Related Work
============

DDPMs for Image Synthesis
-------------------------

Image-Guided Diffusion and Reconstruction
-----------------------------------------

**Conditioning of DDPMs on Accelerated MRI**
Semester Thesis
Lionel Peer
Department of Information Technology and Electrical Engineering

+-----------------+---------------------------------------------------+
| **Advisors:**   | Georg Brunner & Emiljo Mëhillaj                   |
+-----------------+---------------------------------------------------+
| **Supervisor:** | Prof. Dr. Ender Konukoglu                         |
+-----------------+---------------------------------------------------+
|                 | Computer Vision Laboratory, Group for Biomedical  |
|                 | Image Computing                                   |
+-----------------+---------------------------------------------------+
|                 | Department of Information Technology and          |
|                 | Electrical Engineering                            |
+-----------------+---------------------------------------------------+

January 10, 2020
