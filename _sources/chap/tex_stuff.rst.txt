Abstract
========

The abstract gives a concise overview of the work you have done. The
reader shall be able to decide whether the work which has been done is
interesting for him by reading the abstract. Provide a brief account on
the following questions:

-  What is the problem you worked on? (Introduction)

-  How did you tackle the problem? (Materials and Methods)

-  What were your results and findings? (Results)

-  Why are your findings significant? (Conclusion)

The abstract should approximately cover half of a page, and does
generally not contain citations.

Acknowledgements
================

The First Appendix
==================

In the appendix, list the following material:

-  Data (evaluation tables, graphs etc.)

-  Program code

-  Further material

Extended Derivations
====================

.. _app:forward:

Forward Process Closed-Form
---------------------------

Starting with transition distributions

.. math:: q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \bm{x}_{t-1}, \beta_t I)

the reparameterization :math:`\alpha = 1 - \beta` is introduced

.. math:: q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \bm{x}_{t-1}, (1-\alpha) I)

which can also be formulated as

.. math:: q(\bm{x}_t|\bm{x}_{t-1}) = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t}\mathcal{N}(\bm{0}, \bm{I}).

For coherent indexing it is beneficial to switch to notation using
random variables

.. math::

   \bm{x}_{t} = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t}\bm{\epsilon_{t-1}}
       \label{eq:forward_randomvar}

where :math:`\bm{\epsilon_{t-1}} \sim \mathcal{N}(\bm{0}, \bm{I})` and
the earlier :math:`\bm{x}_t` can be recursively inserted into the
formula. Recalling that the sum :math:`Z = X + Y` of two normally
distributed random variables
:math:`X \sim \mathcal{N}(\mu_X, \sigma_Y^2)` and
:math:`Y \sim \mathcal{N}(\mu_Y, \sigma_Y^2)` is again normally
distributed according to
:math:`Z \sim \mathcal{N}(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)`

.. math::

   \begin{aligned}
       x_t & = \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1-\alpha_{t-1}}\bm{\epsilon}_{t-2} \right) + \sqrt{1-\alpha_{t}} \bm{\epsilon}_{t-1} \\
           & = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1})} \bm{\epsilon}_{t-2} + \sqrt{1-\alpha_{t}} \bm{\epsilon}_{t-1}         \\
           & = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1}) + (1-\alpha_{t})} \bm{\bar{\epsilon}}_{t-2}\end{aligned}

where :math:`\bm{\bar{\epsilon}}_{t-2}` is the sum of the random
variables up to :math:`t-2` (again Gaussian). The second term can of
course be simplified to

.. math:: \bm{x}_t = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}} \bm{\bar{\epsilon}}_{t-2}

which is exactly the same form as in
Eq. `[eq:forward_randomvar] <#eq:forward_randomvar>`__. Therefore the
final form is

.. math:: \bm{x}_t = \sqrt{\bar{\alpha}_{t}} \bm{x}_{t-2} + \sqrt{1-\bar{\alpha}_{t}} \bm{\bar{\epsilon}}_{t-2}

with :math:`\bar{\alpha_t} = \prod_{s=1}^{t}\alpha_s` as before.

Conclusion
==========

List the conclusions of your work and give evidence for these. Often,
the discussion and the conclusion sections are fused.

Discussion
==========

The discussion section gives an interpretation of what you have done
:raw-latex:`\cite{day2006wap}`:

-  *What do your results mean?* Here you discuss, but you do not
   recapitulate results. Describe principles, relationships and
   generalizations shown. Also, mention inconsistencies or exceptions
   you found.

-  *How do your results relate to other’s work?* Show how your work
   agrees or disagrees with other’s work. Here you can rely on the
   information you presented in the “related work” section.

-  *What are implications and applications of your work?* State how your
   methods may be applied and what implications might be.

Make sure that introduction/related work and the discussion section act
as a pair, i.e. “be sure the discussion section answers what the
introduction section asked” :raw-latex:`\cite{day2006wap}`.

Experiments and Results
=======================

Describe the evaluation you did in a way, such that an independent
researcher can repeat it. Cover the following questions:

-  *What is the experimental setup and methodology?* Describe the
   setting of the experiments and give all the parameters in detail
   which you have used. Give a detailed account of how the experiment
   was conducted.

-  *What are your results?* In this section, a *clear description* of
   the results is given. If you produced lots of data, include only
   representative data here and put all results into the appendix.

.. _sec:forward_diff_experiments:

Influence of Schedules and Image Size on the Forward Diffusion
--------------------------------------------------------------

Ho et al. had derived a closed form solution to the forward process of
DDPMs and Nichol et al. investigated alternative options for the noise
scheduling. :raw-latex:`\autocite{ho2020denoising,nichol2021improved}`
They concluded that the important parameters to model are not the
variances :math:`\beta` of the transitions, but the variances
:math:`1-\bar{\alpha}` of the closed-form forward process, since they
are the ones responsible for the destruction of information.

They decided to go with a squared cosine function, since this would be
close to linear smooth out towards the critical beginning and end points
of the process. In Fig.\ `5.1 <#fig:alphadash>`__ you can see how
:math:`1-\bar{\alpha}` and :math:`\beta` behave for both approaches. It
is immediately visible that the variances reach the maximum too early
and flatten out for the linear schedule. This leads to the intuition
that the last few steps are not very useful.

.. figure:: images/variance_schedule_alphadash.png
   :alt: Variance Schedule Approaches: Modeling the
   :math:`1-\bar{\alpha}` as an approximate linear function (right
   cosine) and deriving :math:`\beta` (left cosine), or modeling
   :math:`\beta` as a linear function (left linear) and deriving
   :math:`1-\bar{\alpha}`.
   :name: fig:alphadash

   Variance Schedule Approaches: Modeling the :math:`1-\bar{\alpha}` as
   an approximate linear function (right cosine) and deriving
   :math:`\beta` (left cosine), or modeling :math:`\beta` as a linear
   function (left linear) and deriving :math:`1-\bar{\alpha}`.

The intution can experimentally confirmed by measuring how closely we
get to isotropic noise when passing samples through the forward process.
For this a batch of 50 times the same image was passed through the
different steps of the process and the covariance matrix was calculated.
As a metric for how close the covariance matrix was to the identity
covariance matrix of pure i.i.d Gaussian noise, the identity matrix was
subtracted and the mean of the absolute value of the matrix calculated.
The results can be seen in Fig. `5.2 <#fig:noisecloseness>`__ and
confirm the intuition: When using linear scheduling we reach the closest
point to pure noise already after around 600 steps for small images, and
after around 700 for larger images. Cosine scheduling also performs
worse on smaller images than on larger ones, but is still capable
providing value for at least 850 timesteps.

.. figure:: images/frobenius_norm.png
   :alt: Closeness to noise for linear scheduling (left) and cosine
   scheduling (right).
   :name: fig:noisecloseness

   Closeness to noise for linear scheduling (left) and cosine scheduling
   (right).

Introduction
============

Give an introduction to the topic you have worked on:

-  *What is the rationale for your work?* Give a sufficient description
   of the problem, e.g. with a general description of the problem
   setting, narrowing down to the particular problem you have been
   working on in your thesis. Allow the reader to understand the problem
   setting.

-  *What is the scope of your work?* Given the above background, state
   briefly the focus of the work, what and how you did.

-  *How is your thesis organized?* It helps the reader to pick the
   interesting points by providing a small text or graph which outlines
   the organization of the thesis. The structure given in this document
   shows how the general structuring shall look like. However, you may
   fuse chapters or change their names according to the requirements of
   your thesis.

Focus of this Work
------------------

Thesis Organization
-------------------

Materials and Methods
=====================

The objectives of the “Materials and Methods” section are the following:

-  *What are tools and methods you used?* Introduce the environment, in
   which your work has taken place - this can be a software package, a
   device or a system description. Make sure sufficiently detailed
   descriptions of the algorithms and concepts (e.g. math) you used
   shall be placed here.

-  *What is your work?* Describe (perhaps in a separate chapter) the key
   component of your work, e.g. an algorithm or software framework you
   have developed.

Generative Machine Learning
===========================

Bayesian Formulation of Latent Variable Models
----------------------------------------------

Before getting started it is important to define the terms used in the
next sections, since they all stem from Bayesian statistics. The
Bayesian theorem can be written as

.. math:: p(z|x) = \frac{p(x|z)p(z)}{p(x)}

where it is implicitly assumed that :math:`p` is a probability density
function over two continuous random variables :math:`x` and :math:`z`.
The formula holds in general, but in generative modeling and machine
learning it is usually assumed that the letter :math:`z` represents a
random variable in latent space (unobserved) from which – after training
– can be sampled to generate new data, whereas :math:`x` is the random
variable that represents the training samples (observed space).

Using above described ordering, the four terms in this formula use
distinct names:

:math:`p(x)`
   is called the *evidence* or the *marginal likelihood*. It encompasses
   the actual observations of the data.

:math:`p(z)`
   is called the *prior*, since it exposes information on :math:`z`
   before any conditioning.

:math:`p(z|x)`
   is called the *posterior*. It describes the distribution over
   :math:`z` after (*post*) having seen the evidence :math:`x`.

:math:`p(x|z)`
   is called the *likelihood*. It gives the literal likelihood of
   observing an example :math:`x` when choosing the latent space to be a
   specific :math:`z`.

Variational Autoencoders
------------------------

One of the most straightforward examples of a generative model, where
the goal is to find such a latent space representation of the training
sample distribution, is the Variational Autoencoder
(VAE) :raw-latex:`\autocite{kingma2022autoencoding}`. The name of the
VAE stems from the Autoencoder, a network that tries to recreate its
output through a bottleneck and thereby learns a compressed
representation of the
data. :raw-latex:`\autocite{https://doi.org/10.1002/aic.690370209}`
Autoencoders bear similarity to other dimension reduction methods like
Principal Component Analysis (PCA) and therefore were first published
under the name *Nonlinear principal component analysis*. The
*variational* part in the VAE stems from the fact that it does not learn
to recreate input samples, but is rather optimized to represent the
distribution over the training samples as a combination of a
parameterized latent distribution :math:`p_{\theta_z}(z)` and a neural
network mapping :math:`p_{\theta_{NN}}(x|z)` between the latent space
and the sample space. The latent distribution is chosen such that
sampling from it is easy, which allows the neural network to create new
data samples (e.g. a multivariate Gaussian).

Marginalizing
:math:`p_\theta(x) = \int p_{\theta_{NN}}(x|z) p_{\theta_z}(z) dz = \frac{p_{\theta_{NN_{out}}}(x|z) p_{\theta_z}(z)}{p_{\theta_{NN_{in}}}(z|x)}`
requires another approximation of the intractable posterior
:math:`p_{\theta_{NN_{in}}}(z|x)` during training. A schematic of a VAE
is shown in Fig. `8.1 <#fig:vae>`__.

The neural networks usually do not contain stochastic layers, but are
deterministic mappings between latent space and sample space
:math:`x \sim p_{\theta_{NN_{out}}}(x|z) \Rightarrow x = f_{\theta_{NN_{out}}}(z)`.
The hope is, that after training the encoder
:math:`p_{\theta_{NN_{in}}}` can be removed and sampling from :math:`z`
is the same as sampling from :math:`x`.

.. figure:: images/vae.png
   :alt: VAE schematic: :math:`p(x)` is approximated through a latent
   variable model where posterior and likelihood are modeled with neural
   networks and the prior on the latent variable is modeled through a
   simple parameterized distribution (often Gaussian). The hope is, that
   after training, sampling from :math:`p(z)` and passing it through the
   neural network :math:`p_{\theta_{NN}}(x|z)`, is the same as sampling
   from :math:`p(x)`.
   :name: fig:vae

   VAE schematic: :math:`p(x)` is approximated through a latent variable
   model where posterior and likelihood are modeled with neural networks
   and the prior on the latent variable is modeled through a simple
   parameterized distribution (often Gaussian). The hope is, that after
   training, sampling from :math:`p(z)` and passing it through the
   neural network :math:`p_{\theta_{NN}}(x|z)`, is the same as sampling
   from :math:`p(x)`.

Diffusion Denoising Probabilistic Models
----------------------------------------

Diffusion Denoising Probabilistic Models (DDPMs or Diffusion Models) are
a generative model that learn the distribution of images in a training
set. During training, sample images are gradually destroyed by adding
noise over many iterations and a neural network is trained, such that
these steps can be inverted.

As the name suggests, image content is diffused in timesteps, therefore
we use the random variable :math:`\bm{x}_0` to represent our original
training images, :math:`\bm{x}_t` for (partially noisy) images at an
intermediate timestep and :math:`\bm{x}_T` for images at the end of the
process where almost all information has been destroyed and the
distribution :math:`q(\bm{x}_T)` largely follows an isotropic Gaussian
distribution – a Gaussian distribution with the identity matrix as
covariance matrix, but a non-zero means vector.

Once the network is trained to create a less noisy image
:math:`\bm{x}_{t-1}` from :math:`\bm{x}_t`, we should be able to sample
some new :math:`\bm{x}_T` and generate new samples from the training
distribution :math:`q(\bm{x}_0)` by passing it many times through the
network until all noise is removed.

Forward Diffusion Process
~~~~~~~~~~~~~~~~~~~~~~~~~

Mathematical Description
^^^^^^^^^^^^^^^^^^^^^^^^

In order to derive a training objective it is important to understand
the workings of the *forward diffusion process*. During this process,
i.i.d (independent and identically distributed) Gaussian noise is
applied to the image over many discrete timesteps. A *variance schedule*
defines the means at variances (:math:`\sqrt{1-\beta}` and
:math:`\beta`) of the added noise at every
timestep. :raw-latex:`\autocite{ho2020denoising}` The whole process can
be expressed as a Markov chain (depicted in
Fig. `8.2 <#fig:forward_diffusion>`__)

.. math:: q(\bm{x}_T|\bm{x}_0) = q(\bm{x}_0) \prod_{t=1}^{T} q(\bm{x}_{t}|\bm{x}_{t-1})

with the transition distributions
:math:`q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \bm{x}_{t-1}, \beta_t I)`.
An example of iterative destruction of an image by this process is shown
in Fig. `8.3 <#fig:forward_naoshima>`__.

.. figure:: images/forward_diffusion.png
   :alt: Forward Diffusion Process: An image is iteratively destroyed by
   adding normally distributed noise, according to a schedule. This
   represents a Markov process with the transition probability
   :math:`q(\bm{x}_t|\bm{x}_{t-1})`.
   :name: fig:forward_diffusion

   Forward Diffusion Process: An image is iteratively destroyed by
   adding normally distributed noise, according to a schedule. This
   represents a Markov process with the transition probability
   :math:`q(\bm{x}_t|\bm{x}_{t-1})`.

.. figure:: images/forward_naoshima.png
   :alt: Example of Iterative Image Destruction through Forward
   Diffusion Process: The indices give the time step in the iterative
   destruction process, where :math:`\beta` was created according to a
   linear noise variance schedule (5000 steps from in the 0.001 to 0.02
   range and picture resolution of 4016 by 6016 pixels).
   :name: fig:forward_naoshima

   Example of Iterative Image Destruction through Forward Diffusion
   Process: The indices give the time step in the iterative destruction
   process, where :math:`\beta` was created according to a linear noise
   variance schedule (5000 steps from in the 0.001 to 0.02 range and
   picture resolution of 4016 by 6016 pixels).

Gladly it is not necessary to sample noise again and again in order to
arrive at :math:`\bm{x}_t`, since Ho et al. derived a closed-form
solution to the sampling
procedure. :raw-latex:`\autocite{ho2020denoising}` For this, the
variance schedule is first reparameterized as :math:`1-\beta = \alpha`

.. math::

   q(\bm{x}_t | \bm{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \bm{x}_{t-1}, (1-\alpha_t) \bm{I})
       \label{eq:forward_alpha}

and the closed-form solution for :math:`q(\bm{x}_t|\bm{x}_0)` is derived
by introducing the cumulative product
:math:`\bar{\alpha_t} = \prod_{s=1}^{t}\alpha_s` as

.. math::

   q(\bm{x}_t|\bm{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha_t}}\bm{x}_0, (1-\bar{\alpha_t})\bm{I})
       \label{eq:forward_alphadash}

The derivation that leads from
Eq. `[eq:forward_alpha] <#eq:forward_alpha>`__ to
Eq. `[eq:forward_alphadash] <#eq:forward_alphadash>`__ will be left to
appendix `2.1 <#app:forward>`__.

Influence of Scheduling Functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The process of information destruction is dependent on the chosen
variance schedule, the number of steps and the image size. Beyond the
most simple case – a constant variance over time – Ho et al. opted for
the second most simple option, a linear schedule, where the variance
:math:`\beta_t` grows linearly in
:math:`t`. :raw-latex:`\autocite{ho2020denoising}` Nichol et al. later
found that a cosine-based schedule gives better results, since it does
not destruct information quite as quickly, making it more informative in
the last few timesteps. :raw-latex:`\autocite{nichol2021improved}` Own
experiments exploring above mentioned parameters are explained
in `5.1 <#sec:forward_diff_experiments>`__.

Reverse Diffusion Process
~~~~~~~~~~~~~~~~~~~~~~~~~

DDPMs can be viewed as latent space models in a similar way that
Generative Adversarial Nets or Variational Autoencoders
can. :raw-latex:`\autocite{goodfellow2014generative,kingma2022autoencoding}`

In DDPMs the reverse process is again a Markov chain and can therefore
again be factorized

.. math:: q(\bm{x}_0|\bm{x}_T) = q(\bm{x}_T) \prod_{t=T}^{1} q(\bm{x}_{t-1}|\bm{x}_{t})

which means that our network does not learn to approximate the full
inversion, but rather just the transition probabilities
:math:`q(\bm{x}_{t-1}|\bm{x}_{t})` in the chain, which are transitions
between several intermediate latent distributions.

Latent Variable Models Compared
-------------------------------

+------------+-----------------+-----------------+-----------------+
|            | VAE             | GAN             | DDPM            |
+============+=================+=================+=================+
| prior      | :math:`p(z)`,   | :math:`p(z)`,   | :math           |
|            | parameterized   | parameterized   | :`p(\bm{x}_t)`, |
|            | of any shape    | of any shape    | same shape as   |
|            |                 |                 | samples         |
+------------+-----------------+-----------------+-----------------+
| posterior  | :math:`p(z|x)`, | :math:`p(z|x)`, | :ma             |
|            | modeled with    | modeled with    | th:`q(\bm{x}_t| |
|            | neural network  | loss function   | \bm{x}_{t-1})`, |
|            |                 |                 | modeled as step |
|            |                 |                 | in forward      |
|            |                 |                 | diffusion       |
|            |                 |                 | process         |
+------------+-----------------+-----------------+-----------------+
| likelihood | :math:`p(x      | :math:`p(x      | :math:`q(\bm{   |
|            | |z)=f_{NN}(z)`, | |z)=f_{NN}(z)`, | x}_{t-1}|\bm{x} |
|            | modeled with    | modeled with    | _{t}) = \mathca |
|            | neural network  | neural network  | l{N}(k_1 f_{NN} |
|            |                 |                 | , k_2 f_{NN})`, |
|            |                 |                 | modeled with    |
|            |                 |                 | Gaussian        |
|            |                 |                 | sampling with   |
|            |                 |                 | parameters      |
|            |                 |                 | estimated by    |
|            |                 |                 | neural network  |
+------------+-----------------+-----------------+-----------------+

Loss Functions
--------------

In generative machine learning we would want our model to learn the
distribution that generated out training examples. Often this
distribution is conditioned on some description (e.g. text) or on the
corruption process in our case, where we use generative models to solve
inverse problems. Assuming our original data distribution (of images) is
:math:`p(x)`, then we try to find a parameterized variational machine
learning model (:math:`q_{\theta}(x)`) that will closely match the data
distribution.

In order for this :math:`q_{\theta}(x)` to be trained we need a
differentiable loss function that expresses “closeness” in a
distributional sense. The usual approach to this is to use the
Kullback-Leibler (KL) divergence.

Kullback-Leibler Divergence
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Wasserstein Distance
~~~~~~~~~~~~~~~~~~~~

A different approach to comparing the similarity of distributions is the
Wasserstein metric, successfully used in the Wasserstein
GAN. :raw-latex:`\autocite{arjovsky2017wasserstein}`.

Variational Lower Bound
~~~~~~~~~~~~~~~~~~~~~~~

As mentioned previously, the goal is to approximate a true data
distribution :math:`p^*(x)` with a parameterized distribution
:math:`p_\theta(x) = \int p_\theta(x|z)p(z)dz`, from which sampling is
easy, since the prior :math:`p(z)` is very simple.

Related Work
============

Describe the other’s work in the field, with the following purposes in
mind:

-  *Is the overview concise?* Give an overview of the most relevant work
   to the needed extent. Make sure the reader can understand your work
   without referring to other literature.

-  *Does the compilation of work help to define the “niche” you are
   working in?* Another purpose of this section is to lay the groundwork
   for showing that you did significant work. The selection and
   presentation of the related work should enable you to name the
   implications, differences and similarities sufficiently in the
   “discussion” section.

**My first and last thesis**
**Subtitle Subtitle Subtitle**
Master’s Thesis
Eckhart Immerheiser
Department of ...

================= ========================
**A**\ dvisors:   Egon Hasenfratz-Schreier
**S**\ upervisor: Prof. Dr. Luc van Gool
================= ========================

January 10, 2020
