Unconditional DDPMs have the potential to be powerful priors for the
reconstruction of undersampled MRI and this work explores several ways
of conditioning DDPMs for this task. Prior work on image inpainting and
image-to-image translation introduced a way of conditioning DDPMs by
injecting known information into the latent representations, but this
technique failed to translate to the task of MRI reconstruction. In
order to identify issues with this conditioning method, variability in
samples from DDPMs was studied, both in image and frequency space, in
order to determine feature hierachies over the reverse diffusion
process. The experiments did not succeed in alleviating the issues, but
the interpretation of the DDPM as a score-based model offered a way of
integrating DDPM priors into known iterative reconstruction schemes and
produced competitive reconstruction results for high undersampling
factors. The results with *loss guidance* were even improved by
optimizing over more iterations, which results in resampling the DDPM.

I would like to thank my advisors, Emiljo and Georg, for the support,
trust and liberty that I was given over the course of this project. I
was able to freely decide the course of this project, and discussions
and questions were always received with open arms by the them. Further,
I would like to thank Professor Konukoglu for enabling this project in
his group and last but not least my friends & family, who made sure that
I would balance work and leisure.

Extended Derivations
====================

.. _app:forward:

Forward Process Marginal
------------------------

Starting with transition distributions

.. math:: q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \bm{x}_{t-1}, \beta_t \bm{I})

the reparameterization :math:`\alpha = 1 - \beta` is introduced

.. math:: q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \bm{x}_{t-1}, (1-\alpha_t) \bm{I})

which can be reformulated using the reparameterization trick as

.. math::

   \begin{aligned}
       \bm{x}_t & = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t}\cdot\mathcal{N}(\bm{0}, \bm{I}) \\
                & = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t} \cdot \bm{\epsilon}\end{aligned}

with :math:`\bm{\epsilon} \sim \mathcal{N}(\bm{0}, \bm{I})`. For the
derivation it is helpful to use proper indices on the noise variables
:math:`\bm{\epsilon}_t` and track them

.. math::

   \bm{x}_{t} = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t}\bm{\epsilon_{t-1}}.
       \label{eq:forward_randomvar}

The next term :math:`\bm{x}_{t-1}` can now be inserted into the formula
by again using the reparameterization trick. Recalling that the sum
:math:`Z = X + Y` of two normally distributed random variables
:math:`X \sim \mathcal{N}(\mu_X, \sigma_Y^2)` and
:math:`Y \sim \mathcal{N}(\mu_Y, \sigma_Y^2)` is again normally
distributed according to
:math:`Z \sim \mathcal{N}(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)`

.. math::

   \begin{aligned}
       x_t & = \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1-\alpha_{t-1}}\bm{\epsilon}_{t-2} \right) + \sqrt{1-\alpha_{t}} \bm{\epsilon}_{t-1} \\
           & = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1})} \bm{\epsilon}_{t-2} + \sqrt{1-\alpha_{t}} \bm{\epsilon}_{t-1}         \\
           & = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1}) + (1-\alpha_{t})} \bm{\bar{\epsilon}}_{t-2}\end{aligned}

where :math:`\bm{\bar{\epsilon}}_{t-2}` is the noise variable for the
sum of the random random variables up to :math:`t-2` (again
:math:`\bm{\bar{\epsilon}}_{t-2} \sim \mathcal{N}(\bm{0}, \bm{I})`). The
second term can be simplified to

.. math:: \bm{x}_t = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}} \bm{\bar{\epsilon}}_{t-2}

which is exactly the same form as in
Eq. `[eq:forward_randomvar] <#eq:forward_randomvar>`__. The same
procedure can be repeated in a recursive manner until the arrival at

.. math:: \bm{x}_t = \sqrt{\prod_{s=1}^{t}\alpha_s} \bm{x}_{0} + \sqrt{1-\prod_{s=1}^{t}\alpha_s} \bm{\bar{\epsilon}}_{0}.

At this point we define :math:`\bar{\alpha_t} = \prod_{s=1}^{t}\alpha_s`
and arrive at the final forms

.. math::

   \begin{aligned}
       \bm{x}_t                           & = \sqrt{\bar{\alpha}_{t}} \bm{x}_{0} + \sqrt{1-\bar{\alpha}_{t}} \bm{\bar{\epsilon}}_{0} \\
       \Rightarrow q(\bm{x}_t|\bm{x}_{0}) & = \mathcal{N}(\sqrt{\bar{\alpha}_t} \bm{x}_{0}, (1-\bar{\alpha}) \bm{I}).\end{aligned}

Derivation of Reverse Process Parameterization
----------------------------------------------

This derivation follows the work of
Luo :raw-latex:`\autocite{luo2022understanding}` and starts with
applying Bayes’ rule to the forward process transition

.. math:: q(\bm{x}_t|\bm{x}_{t-1}, \bm{x}_0) = \frac{q(\bm{x}_{t-1}|\bm{x}_{t},\bm{x}_{0})q(\bm{x}_{t}|\bm{x}_{0})}{q(\bm{x}_{t-1}|\bm{x}_{0})}

where :math:`q(\bm{x}_t|\bm{x}_{t-1},\bm{x}_{0})` is independent of
:math:`\bm{x}_0` given :math:`\bm{x}_{t-1}` thanks to the factorization
as a Markov chain and therefore

.. math::

   \begin{aligned}
       q(\bm{x}_t|\bm{x}_{t-1})                        & = \frac{q(\bm{x}_{t-1}|\bm{x}_{t},\bm{x}_0)q(\bm{x}_{t}|\bm{x}_{0})}{q(\bm{x}_{t-1}|\bm{x}_{0})}                                                                                                                                           \\
       \Rightarrow q(\bm{x}_{t-1}|\bm{x}_{t},\bm{x}_0) & = \frac{q(\bm{x}_t|\bm{x}_{t-1})q(\bm{x}_{t-1}|\bm{x}_{0})}{q(\bm{x}_t|\bm{x}_{0})}                                                                                                                                                        \\
                                                       & = \frac{\mathcal{N}(\sqrt{1-\beta_t}\bm{x_{t-1}}, \beta_t \bm{I}) \cdot \mathcal{N}(\sqrt{\bar{\alpha}_{t-1}}\bm{x_{t-1}}, (1-\bar{\alpha}_{t-1}) \bm{I})}{\mathcal{N}(\sqrt{\bar{\alpha}_{t}}\bm{x_{t-1}}, (1-\bar{\alpha}_{t}) \bm{I})}.\end{aligned}

The means and variances can be inserted into the formula for the
multivariate i.i.d Gaussian and after extended derivations
(see :raw-latex:`\autocite{luo2022understanding}`, p. 12) one arrives at
the final form

.. math:: q(\bm{x}_{t-1}|\bm{x}_{t},\bm{x}_0) \propto \mathcal{N}\left(\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right) \bm{x}_{t}+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_{t}\right) \bm{x}_{0}}{1-\bar{\alpha}_{t}}, \frac{\left(1-\alpha_{t}\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}} \mathbf{I}\right).

Derivation of ELBO/VLB
----------------------

In the case of a VAE we have

.. math::

   \begin{aligned}
       \log p_{\theta}(x) & = \log p_{\theta}(x) \int p_{\theta_{NN}}(z|x)dz                                                                                                                                                                           \\
                          & = \int \log p_{\theta}(x) p_{\theta_{NN}}(z|x)dz                                                                                                                                                                           \\
                          & = \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log p_{\theta}(x) \right]        \label{eq:A21}                                                                                                                            \\
                          & = \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log \frac{p_{\theta_{NN}}(x|z)p_{\theta_z}(z)}{p(z|x)}\right]   \label{eq:A31}                                                                                             \\
                          & = \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log \frac{p_{\theta_{NN}}(x|z)p_{\theta_z}(z)p_{\theta_{NN}}(z|x)}{p(z|x)p_{\theta_{NN}}(z|x)}\right]                      \label{eq:a35}                                  \\
                          & = \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log \frac{p_{\theta_{NN}}(x|z)p_{\theta_z}(z)}{p_{\theta_{NN}}(z|x)}\right] + \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log \frac{p_{\theta_{NN}}(z|x)}{p(z|x)}\right] \\
                          & = \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log \frac{p_{\theta_{NN}}(x|z)p_{\theta_z}(z)}{p_{\theta_{NN}}(z|x)}\right] + KL \left[p_{\theta_{NN}}(z|x)||p(z|x)\right].\end{aligned}

Realize that only if :math:`p_{\theta_{NN}}(z|x) = p(z|x)` – which is
exactly when the the KL divergence is 0 – we would get our original
marginal log-likelihood :math:`p_{\theta}(x)` from the first term, as
defined in Eq. `[eq:A31] <#eq:A31>`__, by substituting and calculating
back from Eq. `[eq:a35] <#eq:a35>`__.

.. math::

   \begin{aligned}
       \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log \frac{p_{\theta_{NN}}(x|z)p_{\theta_z}(z)}{p_{\theta_{NN}}(z|x)}\right] & \stackrel{p_{\theta_{NN}}(z|x) = p(z|x)}{=} \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\underbrace{\left[\log \frac{p_{\theta_{NN}}(x|z)p_{\theta_z}(z)}{p(z|x)}\right]}_{p_{\theta}(x)} \\
                                                                                                                                 & = \int \log p_{\theta}(x) p_{\theta_{NN}}(z|x) dz                                                                                                                                    \\
                                                                                                                                 & = \log p_{\theta}(x)\end{aligned}

Training Metrics
================

.. figure:: images/dutifulpondmetrics.png
   :alt: Metrics from the Training Process of dutifulpond10
   :name: fig:dutifulpond10

   Metrics from the Training Process of dutifulpond10

.. container::
   :name: tab:dutifulpond10

   .. table:: Overview over the Hyperparameters of dutifulpond10: The
   model was trained on all RSS reconstructions of the fastMRI brain
   dataset at a resolution of :math:`128\times 128`, at a batch size of
   48 and using the Adam optimizer at an initial learning rate of
   0.0001.

      ================== =================================
      Hyperparameter     Value
      ================== =================================
      dataset            utils.datasets.FastMRIBrainTrain
      dropout            0
      backbone           models.unet.UNet
      img_size           128
      attention          False
      loss_func          torch.nn.functional.mse_loss
      optimizer          torch.optim.adam.Adam
      activation         torch.nn.modules.activation.SiLU
      batch_size         48
      in_channels        1
      kernel_size        3
      architecture       models.diffusion.DiffusionModel
      forward_diff       models.diffusion.ForwardDiffusion
      time_enc_dim       512
      learning_rate      0.0001
      max_timesteps      1000
      schedule_type      cosine
      from_checkpoint    False
      mixed_precision    True
      backbone_enc_depth 5
      unet_init_channels 64
      ================== =================================

Samples & Plots
===============

.. figure:: images/gradientspectra.png
   :alt: Values and Spectra of Loss Gradients: Middle and high
   frequencies seem to be particularly relevant around timestep
   :math:`t=250`, which slightly counters the observations from
   section `6.4 <#sec:predvariance>`__.
   :name: fig:lossgradients

   Values and Spectra of Loss Gradients: Middle and high frequencies
   seem to be particularly relevant around timestep :math:`t=250`, which
   slightly counters the observations from
   section `6.4 <#sec:predvariance>`__.

.. figure:: images/directsampling_comparison.png
   :alt: Direct Sampling with Varying Masks and Guidance Factors: a) -
   b) Reconstructions of two samples, reconstructed using different
   masks (vertical) and different guidance factors (horizontal). For
   comparison, the top row is always the original samples. Low guidance
   factors lead to samples with less contrast and less adherence to the
   original, while high guidance factors combined with high
   accelerations can cause aliasing as observed in the lower right
   corner of a).
   :name: fig:directsamplingcomparison

   Direct Sampling with Varying Masks and Guidance Factors: a) - b)
   Reconstructions of two samples, reconstructed using different masks
   (vertical) and different guidance factors (horizontal). For
   comparison, the top row is always the original samples. Low guidance
   factors lead to samples with less contrast and less adherence to the
   original, while high guidance factors combined with high
   accelerations can cause aliasing as observed in the lower right
   corner of a).

.. figure:: images/kspacedistribution.png
   :alt: Noise Distribution of Latent Space and Latent K-Space: a)
   Measured noise variances of forward process for image space as well
   as absolute, real and imaginary parts of k-space. The variances for
   the real and imaginary part are the same and were only shifted for
   visibility. They correspond to half the variances of the image space
   as can be seen in b) or when comparing the histograms in c), e) and
   f). The distribution of the absolute values of k-space in d) follows
   the Rice distribution.
   :name: fig:kspacedistribution

   Noise Distribution of Latent Space and Latent K-Space: a) Measured
   noise variances of forward process for image space as well as
   absolute, real and imaginary parts of k-space. The variances for the
   real and imaginary part are the same and were only shifted for
   visibility. They correspond to half the variances of the image space
   as can be seen in b) or when comparing the histograms in c), e) and
   f). The distribution of the absolute values of k-space in d) follows
   the Rice distribution.

.. figure:: images/samplingstrategies.png
   :alt: Sampling Strategies for DDPMs: This figure illustrates
   different strategies for trading off computation resources and sample
   quality. In order to make the details visible it is shown for a model
   with 500 timesteps, though the models in this work were trained on
   1000 timesteps. Notable in the plot is the short-grained resampling,
   which is the resampling strategy used by Lugmayr et
   al :raw-latex:`\autocite{lugmayr2022repaint}` and they coined the
   terms *jump length* (:math:`j`) for the local region where resampling
   happens and *resamplings* (:math:`r`) for the number of resamplings
   in each localized area. Long-grained resampling repeats the sampling
   process over the last reverse diffusion timesteps, always shrinking
   it by :math:`j` timesteps with every iteration. The hope is that the
   latent representations at the start of a new resamplings are refined
   versions of the predictions from direct sampling at the corresponding
   timestep. By generalizing the time embedding, means and variances, it
   is further possible to make the model generalize to different numbers
   of timesteps, as illustrated with the slowdown and speedup.
   :name: fig:stepsplot

   Sampling Strategies for DDPMs: This figure illustrates different
   strategies for trading off computation resources and sample quality.
   In order to make the details visible it is shown for a model with 500
   timesteps, though the models in this work were trained on 1000
   timesteps. Notable in the plot is the short-grained resampling, which
   is the resampling strategy used by Lugmayr et
   al :raw-latex:`\autocite{lugmayr2022repaint}` and they coined the
   terms *jump length* (:math:`j`) for the local region where resampling
   happens and *resamplings* (:math:`r`) for the number of resamplings
   in each localized area. Long-grained resampling repeats the sampling
   process over the last reverse diffusion timesteps, always shrinking
   it by :math:`j` timesteps with every iteration. The hope is that the
   latent representations at the start of a new resamplings are refined
   versions of the predictions from direct sampling at the corresponding
   timestep. By generalizing the time embedding, means and variances, it
   is further possible to make the model generalize to different numbers
   of timesteps, as illustrated with the slowdown and speedup.

.. figure:: images/t_embedding.png
   :alt: Time Encoding: a) The 3D surface plot of the time encodings
   shows that the low frequencies (encoded in the higher dimensions of
   the encoding) are responsible for differentiating between timesteps
   far apart, high frequencies for close timesteps. b) In order to
   condition the UNet on the time encodings, the encoding and decoding
   blocks include learned linear layers creating embeddings of the same
   dimension as the channels. The time embeddings are then added onto
   the hidden layer via a broadcasting operation.
   :name: fig:timeencoding

   Time Encoding: a) The 3D surface plot of the time encodings shows
   that the low frequencies (encoded in the higher dimensions of the
   encoding) are responsible for differentiating between timesteps far
   apart, high frequencies for close timesteps. b) In order to condition
   the UNet on the time encodings, the encoding and decoding blocks
   include learned linear layers creating embeddings of the same
   dimension as the channels. The time embeddings are then added onto
   the hidden layer via a broadcasting operation.

Conclusion
==========

List the conclusions of your work and give evidence for these. Often,
the discussion and the conclusion sections are fused.

Discussion and Conclusion
=========================

The focus of this work was on understanding DDPMs and conditioning them
for the task of reconstructing undersampled MRI. The modeling and loss
functions of the DDPM were derived in great detail and a package was
created that implements DDPMs from scratch and provides the necessary
utilities for efficient training, logging and sampling. Prior work by
Lugmayr et al. and Choi et
al. :raw-latex:`\autocite{lugmayr2022repaint,choi2021ilvr}` had
introduced a conditioning method for inpainting and image-to-image
translation respectively that used the known posterior of the forward
process to substitute predicted information with known information in
the latent space. Both their works, RePaint and ILVR, were successfully
implemented with a model trained on MRI data and resampling for RePaint
was equally observed to improve semantics of the reconstruction. Lugmayr
et al.’s work was subsequently adapted to the task of MRI
reconstruction, but the direct adaptation was shown to produce
insufficient reconstructions. In order to avoid image artifacts, like
aliasing and ringing, Choi et al.’s filtering was reintroduced in the
form of a scheduled Gaussian filter that only conditions the model on
low frequencies early in the reverse diffusion process and introduces
higher frequencies later. The intuition behind this approach was that
low frequencies would carry very little information for high noise
variances of the latent space, since this is a general property of
natural images. While reconstruction quality failed to improve through
the use of filtering, the analysis of outcome variability offered a
unique view on the hierarchy of features in a DDPM. As hypothesized,
global image features corresponding to low frequencies were determined
early in the DDPM, whereas high frequency showed variety until very late
in the denoising process.

Using the score-based interpretation of the DDPM and adapting classifier
guidance to accomodate other types of data consistency functions proved
to be a much more flexible and powerful approach to the conditioning
problem. *Loss guidance* performed very well out of the box for the
lower accelerations in the range :math:`\approx 3-6`, with high
perceptual sample quality and very good MSE scores. For the highest
acceleration of :math:`>11`, direct sampling often produced aliasing
artifacts, indicating that the final prediction did not correspond to
the same distributional mode as the samples used for guidance, which led
to frequency mismatch. Using the *long-grained* resampling technique,
this issue was resolved and the very high acceleration of :math:`>11`
produced reconstructions that almost reached the quality of the ones
with direct sampling and acceleration :math:`\approx 5.5`.

Loss guidance proved to be a powerful tool for including DDPMs into
iterative reconstruction schemes and resampling allowed for the increase
of iteration steps, that was helpful for the most challenging
reconstruction tasks. Since the focus of this work was on finding
appropriate conditioning methods, using a set of dedicated test images
was not the first priority, but future work should investigate how well
this method generalizes to unseen images. The uncertainty of the
predictions was also not investigated in this work and since loss
guidance coincides with MAP estimation, it might be interesting to
introduce additional regularizers that can lower the uncertainty.
Finally, models capable of higher resolution should be trained and
evaluated. The highest resolution used in this work was
:math:`128\times 128` and section `8.3 <#sec:networkarch>`__ already
presented ideas how fully convolutional architectures could be trained
to generalize on higher image resolutions.

.. _sec:experimentsandresults:

Experiments and Results
=======================

Training Models on MNIST, CIFAR and fastMRI
-------------------------------------------

In order to explore hyperparameters of the model architecture and debug
the implementation, it was decided to first train models on datasets
considered trainable with less computation time. Two very well known
datasets in computer vision that use low-resolution images are CIFAR10
and MNIST. :raw-latex:`\autocite{cifar,mnist}`

.. figure:: images/cifarsamples.png
   :alt: Samples from the best performing model trained on CIFAR10 using
   a linear schedule: While the variance in the samples is large,
   suggesting that the model is able to capture the whole distribution,
   the samples are not completely denoised and therefore sample quality
   is seriously degenerated. As can be seen later, this was not observed
   when training on datasets where the image resolution was higher.
   :name: fig:cifarsamples

   Samples from the best performing model trained on CIFAR10 using a
   linear schedule: While the variance in the samples is large,
   suggesting that the model is able to capture the whole distribution,
   the samples are not completely denoised and therefore sample quality
   is seriously degenerated. As can be seen later, this was not observed
   when training on datasets where the image resolution was higher.

.. figure:: images/mnistsamples.png
   :alt: Samples from the best performing model trained on MNIST.
   :name: fig:mnistsamples

   Samples from the best performing model trained on MNIST.

Unconditional sampling was performed in order to verify the sample
quality and whether the model was able to capture the main modes of the
training data distribution. For a quantitative analysis of sample
quality and mode coverage/log-likelihood of trained models, Nichol et
al. use FID score and log-likelihood estimates. FID requires an
additional classifier network, which only makes sense on standardized
datasets with class labels such as ImageNet or
CIFAR :raw-latex:`\autocite{imagenet, cifar}`, where pretrained
classifier weights are usually available, making scores comparable among
different generative models. The fastMRI dataset is not meant for
classification tasks, hence not containing any labels that could be used
for such an evaluation. Getting robust estimates of the log-likelihood
on the other hand requires evaluating the model on a significant portion
of the training dataset, which was also omitted due to computational
constraints. Instead, the diversity and quality of the samples was
judged only visually and unconditional samples from 3 trained models can
be seen in . All of those 3 models show good diversity among the
samples, indicating good mode coverage, but while the perceptual quality
of the samples is good for MNIST and fastMRI, the CIFAR10 samples are
unsatisfactory. It might be more difficult to train DDPMs on resolutions
significantly lower than :math:`64\times 64` as will be explored in the
next section, and the only reason why the MNIST model managed to produce
samples of high quality might be due to the comparative simplicity of
the dataset.

The samples from Fig. `6.3 <#fig:uncondsampling>`__ are sampled from a
model trained on fastMRI RSS reconstruction at :math:`128\times 128` and
that specific checkpoint was reached after 60 epochs or around 40’000
steps with a batch size of 48. This model carries the unique identifier
``dutifulpond`` and was used for all subsequent experiments in the
following sections. The exact hyperparameters are listed in
Table `2.1 <#tab:dutifulpond10>`__ and an overview over the training
process is shown in Fig. `2.1 <#fig:dutifulpond10>`__. At this point it
should be noted that subsequent reconstructions used images from the
training set for guidance. Due to the high stochasticity of the DDPM and
the very strong undersamplings that were used in this work, it is still
a challenging problem to guide an unconditional DDPM to a good
reconstruction of the input as will be demonstrated in the coming
sections.

.. figure:: images/samples_unconditional.png
   :alt: a) Unconditionally sampled examples, produced by the
   best-performing model ``dutifulpond``. b) Examples from the training
   data set (fastMRI, RSS reconstruction). As can be seen, sample
   quality is comparable to the quality of the training images and the
   variability among the samples is high, indicating a decent mode
   coverage of the model.
   :name: fig:uncondsampling

   a) Unconditionally sampled examples, produced by the best-performing
   model ``dutifulpond``. b) Examples from the training data set
   (fastMRI, RSS reconstruction). As can be seen, sample quality is
   comparable to the quality of the training images and the variability
   among the samples is high, indicating a decent mode coverage of the
   model.

.. _sec:forward_diff_experiments:

Influence of Schedules and Image Size on the Forward Diffusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ho et al. had derived a closed form solution to the forward process of
DDPMs and Nichol et al. investigated alternative options for the noise
scheduling. :raw-latex:`\autocite{ho2020denoising,nichol2021improved}`
They concluded that the important parameters to model are not the
variances :math:`\beta` of the transitions, but the variances
:math:`1-\bar{\alpha}` of the closed-form forward process, since they
are the ones responsible for the destruction of information.

They decided to use a squared cosine function for the
:math:`1-\bar{\alpha}`, since this would be close to linear in the
middle and would smoothly transition to constants towards the critical
beginning and end points of the process. Fig.\ `6.4 <#fig:alphadash>`__
shows how :math:`1-\bar{\alpha}` and :math:`\beta` behave for the two
approaches of either modeling the :math:`\beta` linearly, or modeling
the :math:`1-\bar{\alpha}` according to a squared cosine. It is
immediately visible that the variances of the linear schedule reach the
maximum too early and flatten out, leading to the intuition that the
last few steps are not very useful.

.. figure:: images/variance_schedule_alphadash.png
   :alt: Variance Schedule Approaches: Modeling the
   :math:`1-\bar{\alpha}` as an approximate linear function (right
   cosine) and deriving :math:`\beta` (left cosine), or modeling
   :math:`\beta` as a linear function (left linear) and deriving
   :math:`1-\bar{\alpha}`.
   :name: fig:alphadash

   Variance Schedule Approaches: Modeling the :math:`1-\bar{\alpha}` as
   an approximate linear function (right cosine) and deriving
   :math:`\beta` (left cosine), or modeling :math:`\beta` as a linear
   function (left linear) and deriving :math:`1-\bar{\alpha}`.

The intution can experimentally confirmed by measuring how closely the
latent representations go to isotropic noise when passing samples
through the forward process. For this a batch of 50 times the same image
:math:`x_0` was passed through the forward process and the covariance
matrix over all :math:`x_T` was calculated. As a metric for how close
the covariance matrix was to the identity covariance matrix of pure
i.i.d Gaussian noise, the identity matrix was subtracted and the mean of
the absolute value of the matrix calculated. The experiment was
conducted at several image resolutions, since Nichol et al. found the
cosine schedule to be superior mainly for low resolution images. The
results can be seen in Fig. `6.5 <#fig:noisecloseness>`__ and confirm
the intuition: When using linear scheduling the closest point to pure
noise is already reached after around 600 steps for small images, and
after around 700 for larger images. Even more interesting is the fact
that both schedules struggle to reach pure noise for the lowest two
resolutions, which might be a reason for the poor sample quality reached
on the CIFAR10 dataset, that was shown in the previous section.

.. figure:: images/frobenius_norm.png
   :alt: Closeness to i.i.d Gaussian noise for linear scheduling (left)
   and cosine scheduling (right).
   :name: fig:noisecloseness

   Closeness to i.i.d Gaussian noise for linear scheduling (left) and
   cosine scheduling (right).

Image Inpainting & Low-Frequency Guidance
-----------------------------------------

The works by Lugmayr et al. :raw-latex:`\autocite{lugmayr2022repaint}`
and Choi et al. :raw-latex:`\autocite{choi2021ilvr}` were the primary
motivation behind pursuing an approach of conditioning unconditionally
trained DDPMs. As a first step it was therefore important to recreate
their results before adapting them to the task of undersampled MRI. The
update steps of the reverse diffusion process were formulated according
to the formulas from section `8.2 <#sec:freqreplacement>`__ and the
results can be seen in Fig. `6.6 <#fig:repaint>`__ and
Fig. `6.7 <#fig:ilvr>`__ for RePaint and ILVR respectively.

Using Lugmayr et al.’s suggested hyperparameters for resampling (jump
length of 10, with 10 resamplings) was indeed observed to help with
creating semantically meaningful
reconstructions :raw-latex:`\autocite{lugmayr2022repaint}`, with the
exception of a single sample (lowest row, second from left). This
specific sample also caused issues with ILVR, indicating that the
distributional mode of that image type was not learned well by the
model. Since it is an unusual type of image, it was likely not
represented well in the training data.

.. figure:: images/repaint.png
   :alt: Inpainting and Resampling: a) Masked images used for the
   RePaint-style conditioning of the unconditional DDPM. b) Results from
   direct sampling. The reconstructed areas are often semantically
   wrong. c) Results from sampling with resampling with jump length
   :math:`j=10` and resamplings :math:`r=10` as suggested by Lugmayrs et
   al :raw-latex:`\autocite{lugmayr2022repaint}`. As observed in their
   work, the resampling strategy helps with semantically meaningful
   reconstruction. d) Ground truth images for comparison.
   :name: fig:repaint

   Inpainting and Resampling: a) Masked images used for the
   RePaint-style conditioning of the unconditional DDPM. b) Results from
   direct sampling. The reconstructed areas are often semantically
   wrong. c) Results from sampling with resampling with jump length
   :math:`j=10` and resamplings :math:`r=10` as suggested by Lugmayrs et
   al :raw-latex:`\autocite{lugmayr2022repaint}`. As observed in their
   work, the resampling strategy helps with semantically meaningful
   reconstruction. d) Ground truth images for comparison.

While Choi et al. used a linear filter consisting of downsampling and
upsampling operations, the filter used in this work was a Gaussian
kernel. As can be seen in the samples of Fig. `6.7 <#fig:ilvr>`__, the
model produces final images that match the rough shape of the guidance
examples, but often differ substantially in the details. In many
instances the type of the final image is not even the same as the
guidance type. As would be expected, this was not observed when using
less blurry images for the guidance.

.. figure:: images/ilvr.png
   :alt: ILVR – Low-Frequency Guidance: a) Original guidance images. b)
   Filtered version used for guidance. c) Final predictions using the
   filtered guidance images. As can be seen, the predictions are of good
   perceptual quality, but the predictions often do not match the ground
   truth images, indicating that the filtering was too strong in this
   specific case.
   :name: fig:ilvr

   ILVR – Low-Frequency Guidance: a) Original guidance images. b)
   Filtered version used for guidance. c) Final predictions using the
   filtered guidance images. As can be seen, the predictions are of good
   perceptual quality, but the predictions often do not match the ground
   truth images, indicating that the filtering was too strong in this
   specific case.

Masked K-Space Substitution
---------------------------

As introduced in section `8.2 <#sec:freqreplacement>`__, ILVR and
RePaint can be used to replace parts of k-space in the prediction
according to

.. math::

   \label{eq:kspacesubstituion}
       x_{t-1} = x_t - \mathcal{F}^{-1}\left(\mathcal{M}\circ\mathcal{F}(x_t) + \mathcal{M}(s_t)\right)

with :math:`\mathcal{M}` being a masking operation and :math:`s_t` being
the latent representation of the known k-space. The latent :math:`s_t`
can be derived from :math:`s_0` by applying noise either in the image
space or directly in k-space and results using both techniques for MRI
reconstruction are depicted in Fig. `6.8 <#fig:freqreplacement>`__. The
sample quality is significantly worse than for ILVR or RePaint, which is
surprising since a relatively low acceleration of factor
:math:`\approx 4.12` was used. The samples suffer from noticeable
aliasing artifacts and unsharp edges, which might stem from the model
being guided into the wrong ditributional mode and the fact that k-space
masks are multiple passband box filters that could induce ringing
artifacts. Aliasing and ringing are usually avoided by using filters and
the simplest choice of filter that avoids such artifacts is a Gaussian
kernel. Gaussian filters are also linear filters, which means that they
integrate well into the framework from
Eq. `[eq:kspacesubstituion] <#eq:kspacesubstituion>`__. The downside of
applying a Gaussian filter are twofold: 1. Discarding higher frequencies
would be wasteful, since they consumed acquisition time; 2. The results
from ILVR showed that low frequency guidance can guide the model to a
different distributional mode than the target image, which indicates
that the high frequencies are not only important for matching the
smallest details. The next section will explore the idea of adding
information gradually over the reverse diffusion process, making higher
frequencies available to the model, but only towards the end of the
process. Since this allows the use of smooth filters like a Gaussian, it
was hypothesized to partially solve the issues of unsharp edges and
aliasing.

RePaint-style resampling was mainly developed to help with matching
semantics therefore it was not expected to yield similar performance
improvements for MRI reconstruction. Nevertheless, the resampling
technique was also used together with the update step from
Eq. `[eq:kspacesubstituion] <#eq:kspacesubstituion>`__ in order to see
if sample quality can be improved through increased computation time,
but this was not the case.

.. figure:: images/freq_replacement.png
   :alt: Diffusion guidance using frequency replacement: a) Corrupted
   samples from an acceleration :math:`\approx 4.12`. b) Results from
   applying noise directly on k-space
   (see `8.2 <#sec:freqreplacement>`__). c) Results from applying noise
   in the image space. The reconstruction quality is similar in both
   cases, but significantly worse than experiments with RePaint and ILVR
   from the previous section would suggest. The reconstructions show
   aliasing artifacts and possibly ringing artifacts at the edges, which
   indicate serious frequency mismatch.
   :name: fig:freqreplacement

   Diffusion guidance using frequency replacement: a) Corrupted samples
   from an acceleration :math:`\approx 4.12`. b) Results from applying
   noise directly on k-space (see `8.2 <#sec:freqreplacement>`__). c)
   Results from applying noise in the image space. The reconstruction
   quality is similar in both cases, but significantly worse than
   experiments with RePaint and ILVR from the previous section would
   suggest. The reconstructions show aliasing artifacts and possibly
   ringing artifacts at the edges, which indicate serious frequency
   mismatch.

.. _sec:predvariance:

Variance in Predictions and Filtered Diffusion
----------------------------------------------

Since the SNR (signal-to-noise-ratio) in natural images is much higher
in the lower frequencies than in the higher ones, it was hypothesized
that higher frequencies carry very little information early in the
reverse diffusion process and it might therefore be possible to add
frequency information gradually during the denoising process. Lower
frequencies first to provide more broad information and higher
frequencies only at the end to make sure that the details of the image
match. Since this could be done by applying filters of varying pass
bands, it might partially solve the issues of aliasing and ringing.

In order to empirically study the relevance of the frequencies, a single
sample was denoised and its latent representations at every 10th
timestep were saved. These latent representations were copied into
batches of 100 equal latent representations and the denoising process
was continued for all these batches. Fig. `6.9 <#fig:predvariance>`__
shows 4 of these samples for a subset of starting points. As can be
seen, when starting from :math:`t\geq700`, the samples still have a lot
of variability and share very few common features. When starting later
in the process, the samples clearly stem from the same distributional
mode and only differ in the details. Since high frequencies are
responsible for carrying information on details, this supports the
hypothesis, but it becomes more evident, when looking at the variances
of the spectral representations as seen in
Fig. `6.10 <#fig:spectralvariance>`__. The variances were estimated over
the frequency representations of all the final predictions in a batch
(100 samples, denoised from a starting point :math:`t`) and high
variance in a frequency indicates that the value of this frequency was
not yet determined at starting point :math:`t`. As can be clearly seen
in the figure, the variance is concentrated in the low frequencies when
starting from a large :math:`t` and the variance of the low frequencies
only becomes comparable to it when starting late in the process (small
:math:`t`). This again supports the hypothesis that high frequencies
matter much more towards the end and that it might be possible to only
introduce them later in the process.

.. figure:: images/fixedlatents_variance.png
   :alt: Variance in Predictions from Fixed Latents: The general shape
   of the final samples is already determined at :math:`t=500` and from
   there, the variability in the outputs is mostly about details of the
   structure.
   :name: fig:predvariance

   Variance in Predictions from Fixed Latents: The general shape of the
   final samples is already determined at :math:`t=500` and from there,
   the variability in the outputs is mostly about details of the
   structure.

.. figure:: images/fixedlatents_varSpectra.png
   :alt: Variance in the Spectra when starting from Fixed Latents: When
   generating samples from fixed latent representation early in the
   denoising process, the variance of the spatial frequencies is highly
   concentrated in the center (e.g. :math:`t=860`), overpowering the
   variance in the low frequencies by several orders of magnitude. The
   differences are smaller when starting late in the process (e.g.
   :math:`t=10`), suggesting that fine details are only reconstructed at
   the very end. This hypothesis is also supported by the fact that
   natural images have lower SNR in the higher frequencies, which means
   that Gaussian perturbation affects them more and they can’t carry
   much information until most of the noise is removed.
   :name: fig:spectralvariance

   Variance in the Spectra when starting from Fixed Latents: When
   generating samples from fixed latent representation early in the
   denoising process, the variance of the spatial frequencies is highly
   concentrated in the center (e.g. :math:`t=860`), overpowering the
   variance in the low frequencies by several orders of magnitude. The
   differences are smaller when starting late in the process (e.g.
   :math:`t=10`), suggesting that fine details are only reconstructed at
   the very end. This hypothesis is also supported by the fact that
   natural images have lower SNR in the higher frequencies, which means
   that Gaussian perturbation affects them more and they can’t carry
   much information until most of the noise is removed.

A gradual introduction of frequencies could be done by introducing a
schedule of k-space masks, but this would again equate to box filters,
which should be avoided for the reasons mentioned in the previous
section. Instead, a schedule of standard deviations for the 1D Gaussian
filter was used and the time dependent filter
(:math:`\phi \rightarrow \phi(t)`) was added to
Eq. `[eq:kspacesubstituion] <#eq:kspacesubstituion>`__ as

.. math:: x_{t-1} = x_t - \mathcal{F}^{-1}\left(\phi(t)\circ\mathcal{M}\circ\mathcal{F}(x_t) + \phi(t)\circ\mathcal{M}(s_t)\right).

Results with such scheduled filters were in general unsatisfying with
some samples showing a slight improvement over unfiltered frequency
replacement, while in others, the aliasing issue was actually amplified,
as demonstrated in Fig. `6.11 <#fig:filtereddiffusion>`__. The search
space over different schedules that could improve the outcome is very
large and since loss guidance (`8.1.2 <#sec:lossguidance>`__) had been
identified as a very flexible and powerful approach at this point,
optimization of the scheduling or resampling strategies using filter
schedules were not further investigated. Loss guidance also offered
another possibility of inspecting dominant frequencies in the guidance
process and the results of this experiment are shown in
Fig. `3.1 <#fig:lossgradients>`__.

.. figure:: images/filtereddiffusion.png
   :alt: Results from scheduled filtering: a) Results from using
   Gaussian filters during reverse diffusion, scheduled according to the
   standard deviations in b) (lower). While contrast is in general
   enhanced compared to simple frequency replacement, and some samples
   show better perceptual quality, aliasing is amplified for others.
   While this was not further investigated, it may be that those samples
   were initially guided into a wrong distributional mode in which later
   introduced frequencies matched even worse.
   :name: fig:filtereddiffusion

   Results from scheduled filtering: a) Results from using Gaussian
   filters during reverse diffusion, scheduled according to the standard
   deviations in b) (lower). While contrast is in general enhanced
   compared to simple frequency replacement, and some samples show
   better perceptual quality, aliasing is amplified for others. While
   this was not further investigated, it may be that those samples were
   initially guided into a wrong distributional mode in which later
   introduced frequencies matched even worse.

Loss Function Guidance
----------------------

Direct Sampling
~~~~~~~~~~~~~~~

Taking gradient steps in the direction of a loss gradient immediately
proved to be a more flexible approach with much better reconstruction
results than the previous methods. Since PyTorch’s auto-differentiation
can not only calculate gradients with respect to weights, but also with
respect to inputs, the calculation of the loss gradients was simple to
implement as can be seen in
Listing `[lst:lossgradient] <#lst:lossgradient>`__. Since prediction and
ground truth k-space are masked, the loss is only influenced by the
known areas, which is why it needs to be scaled by the acceleration.
Otherwise loss values are not comparable between different acceleration
masks and suitable guidance factors would have to be derived
individually for every mask. As mentioned
in `8.1.2 <#sec:lossguidance>`__, this approach would naturally extend
to other types of loss functions like :math:`\text{L}_1` and MSE was
chosen for the similarity between its gradient and the replacement
strategy (section `8.2 <#sec:freqreplacement>`__).

.. code:: iPython

   def mse_grad(pred: Tensor, corrupted_kspace: Tensor, mask: Tensor) -> Tuple:
           pred = pred.requires_grad_(requires_grad=True)

           # corrupted_kspace is already masked
           pred_masked_kspace = to_kspace(pred) * mask
           loss = mse_loss(pred_masked_kspace, corrupted_kspace)

           # loss scaling for different masks
           eff_acc = torch.mean(mask.view(-1))
           loss = loss / eff_acc

           grads = torch.autograd.grad(loss, pred)[0]
           pred = pred.requires_grad_(requires_grad=False)

           return grads, loss.item()

Results that use loss guidance for the reconstruction can be seen in
Fig. `6.12 <#fig:reconstructionslossguidance>`__ and they are of very
high quality for the lower accelerations of :math:`3.12` and
:math:`5.57`, but drop when speeding up acquisition by a factor of
:math:`11.64`. The magnitude of the guidance factor is responsible for
balancing the prior and the data consistency, and this is especially
visible for the highest acceleration, where samples guided by
:math:`g=20'000` have much higher perceptual quality than samples guided
by :math:`g=200'000`. A broader view on the tradeoff between sample
fidelity and data consistency can be viewed in
Fig. `3.2 <#fig:directsamplingcomparison>`__, where a larger range of
guidance factors is considered and the tradeoff becomes more evident.
For very low guidance factors (:math:`\approx 1000`), it can further be
observed that the samples lack contrast. Similar observations were made
by Dhwariwal et al., where classifier guidance yielded signficantly
improved contrast over unconditional
sampling. :raw-latex:`\autocite{dhariwal2021diffusion}`

.. figure:: images/directsampling_small.png
   :alt: Reconstructions using Loss Guidance: a) - d) Reconstructions
   for three different accelerations and two different guidance factors.
   The quality of the reconstructions is usually good for the two lower
   accelerations, but drops significantly for the very high acceleration
   of :math:`11.64`. The reconstructions with the highest acceleration
   also show how the guidance factor can be used to trade off data
   consistency for sample quality, with the lower guidance yielding
   visually pleasing images with fewer artifacts.
   :name: fig:reconstructionslossguidance

   Reconstructions using Loss Guidance: a) - d) Reconstructions for
   three different accelerations and two different guidance factors. The
   quality of the reconstructions is usually good for the two lower
   accelerations, but drops significantly for the very high acceleration
   of :math:`11.64`. The reconstructions with the highest acceleration
   also show how the guidance factor can be used to trade off data
   consistency for sample quality, with the lower guidance yielding
   visually pleasing images with fewer artifacts.

In order to evaluate the reconstruction quality empirically, the mean
squared error to the ground truth was calculated and the results for
different accelerations can be seen in
Fig. `6.13 <#fig:lossguidancelosses>`__ a), where the results were
averaged over 100 different samples. For the lower accelerations, the
MSE continually decreased with increasing guidance and no point was
reached where guidance allocated too much weight to the data
consistency. For the highest acceleration, the loss essentially stops
decreasing at :math:`g=50'000`, with slight fluctuations in the losses
for higher guidance values. This fits the observations from
Fig. `6.12 <#fig:reconstructionslossguidance>`__ and
Fig. `3.2 <#fig:directsamplingcomparison>`__, where higher guidance
values usually led to aliasing. Fig. `6.13 <#fig:lossguidancelosses>`__
b) - d) also includes plots of the running losses for the different
accelerations and guidance factors. In contrast to the final MSE, this
running loss is calculated in k-space and is weighted by the mask
coverage (see Listing `[lst:lossgradient] <#lst:lossgradient>`__). While
it is unsurprising that higher guidance leads to a faster decrease of
the MSE, it is counterintuitive at first that the losses decrease faster
for the highest acceleration, since it provides less guidance
information. The explanation likely lies in the fact that high
acceleration masks sample mainly in low frequencies and the loss is
therefore concentrated in the center of k-space. As discussed
in `6.4 <#sec:predvariance>`__, low frequencies have higher SNR and
having only those available early for guidance might lead to less noisy
gradients early in the reverse diffusion process and subsequent faster
convergence.

.. figure:: images/direct_sampling.png
   :alt: Results from Direct Sampling with Loss Guidance: a) Final MSE
   of the reconstructions for various guidance factors and
   accelerations. b) - d) Plots of running losses, clipped at 0.1, for
   various guidance factors and accelerations. Counterintuitively,
   losses decrease faster for higher accelerations, which is likely due
   to the fact that higher accelerations concentrate losses in central
   k-space, which is determined earlier than the outer regions.
   :name: fig:lossguidancelosses

   Results from Direct Sampling with Loss Guidance: a) Final MSE of the
   reconstructions for various guidance factors and accelerations. b) -
   d) Plots of running losses, clipped at 0.1, for various guidance
   factors and accelerations. Counterintuitively, losses decrease faster
   for higher accelerations, which is likely due to the fact that higher
   accelerations concentrate losses in central k-space, which is
   determined earlier than the outer regions.

Plots of the gradients and of the dominant frequencies for guidance can
be seen in Fig. `3.1 <#fig:lossgradients>`__. Since these gradients are
very noisy, they were averaged over a batch of 1200 equal images. The
spectra of the gradients show that lower frequencies indeed guide more
strongly in the beginning, but contrary to the observations from
section `6.4 <#sec:predvariance>`__, they do not dominate the higher
frequencies, which already have significant values early on.

Increased Computation Time for Better Reconstruction Quality
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As introduced in `8.4 <#sec:morecompute>`__, DDPMs offer several ways of
using more computing resources for potentially higher sample quality.
For the lower accelerations this was of minor interest since their
reconstructions were never observed to suffer from artifacts and no
tradeoff between data consistency and prior was ever necessary, even for
very large guidance factor. For the highest acceleration, the
long-grained resampling proved especially helpful, producing alias-free
reconstructions of high quality for all inspected samples. This also
expressed itself in a significant drop in MSE: For the direct sampling
scheme, the best MSE at acceleration :math:`\approx 11.64` was 0.0052 at
:math:`g=200'000` and it dropped to 0.0016 for long-grained resampling
with :math:`j=100` and :math:`g=20'000`. This MSE is almost in the same
order as the one for direct sampling with acceleration
:math:`\approx 5.57`, which was 0.0011 at :math:`g=20'000`. The
experiment was also repeated for the lower accelerations, but their
reconstructions did not profit from the additional computation time.
Reconstructed samples for the highest acceleration (11.64) and several
guidance factors and jump lengths can be viewed in
Fig. `6.14 <#fig:longgrainedmask2>`__. The measured MSEs of the final
predictions are shown in Fig. `6.15 <#fig:mselonggrained>`__.

.. figure:: images/msk2_longRes.png
   :alt: Comparison of Reconstruction from High Acceleration with
   Long-Grained Resampling: Resampling has a significant impact on the
   reconstruction quality for the high acceleration of
   :math:`\approx 11.64` as is demonstrated by these 4 samples for
   several guidance factors and several jump lengths.
   :name: fig:longgrainedmask2

   Comparison of Reconstruction from High Acceleration with Long-Grained
   Resampling: Resampling has a significant impact on the reconstruction
   quality for the high acceleration of :math:`\approx 11.64` as is
   demonstrated by these 4 samples for several guidance factors and
   several jump lengths.

.. figure:: images/globalresamplingmse.png
   :alt: a) - c) MSEs of reconstructions when using long-grained
   resampling for several accelerations, guidance factors and jump
   lengths. The values are comparable to direct sampling for the two
   lower accelerations, but are significantly better for the highest
   acceleration at :math:`g=5000` and :math:`j=500`.
   :name: fig:mselonggrained

   a) - c) MSEs of reconstructions when using long-grained resampling
   for several accelerations, guidance factors and jump lengths. The
   values are comparable to direct sampling for the two lower
   accelerations, but are significantly better for the highest
   acceleration at :math:`g=5000` and :math:`j=500`.

Introduction
============

Background & Relevance
----------------------

MRI (magnetic resonance imaging) is a medical imaging modality that
allows acquiring slices of the body, which is an invaluable non-invasive
procedure in medical diagnostics. In contrast to the widely used CT
(computed tomography) it does not rely on harmful ionizing radiation and
it offers much better soft tissue contrast. This is enhanced by the
large flexibility of the acquisition protocols, which can often be tuned
to yield the best contrast between tissues of interest. The biggest
difficulty with MRI scans are the long acquisition times, which requires
patients to lay still for extended amounts of time, which is especially
difficult for children and intellectually disabled patients.
Additionally, long acquisition times make scans more expensive and
available to a smaller number of patients. A significant part of
MRI-related research is therefore occupied by reaching acquisition
speedups. Such techniques usually rely on several acquisition
coils :raw-latex:`\autocite{sodickson1997smash,pruessmann1999sense,griswold2002grappa}`
and on undersampling of the acquisition space, which is the space of
spatial frequencies in the case of MRI. This space corresponds to the 2D
Fourier transform of the image space and is usually termed *k-space*,
relating to the variable :math:`k`, the wave number or spatial
frequency. Undersampling k-space poses a challenging inverse problem
that can be solved well by compressed sensing
techniques :raw-latex:`\autocite{donoho2006compressedsensing,candes2005stable}`
for small accelerations (undersampling factors), but needs additional
information from multiple coils for higher accelerations or has to rely
on strong priors.

Generative machine learning for images has made huge progress in the
last few years, thanks to the incorporation of neural networks and since
generative machine learning is concerned with learning data
distributions, it offers a possibility for incorporating such strong
priors into inverse problems. Among the most influential architectures
of the past few years are variational autoencoders (VAEs), generative
adversarial networks (GANs) and diffusion denoising probabilistic models
(DDPMs). :raw-latex:`\autocite{kingma2013autoencoding,goodfellow2014generative,sohldickstein2015deep,ho2020denoising}`

Focus of this Work
------------------

By merging the VAE’s strong mode coverage with sample quality comparable
to GANs, DDPMs have recently emerged as the most powerful model for
modeling image
distributions :raw-latex:`\autocite{dhariwal2021diffusion}` and are
therefore used in this work. Using large amounts of high-quality MRI
data from various acquisition protocols, the focus of this work is on
training strongly generalizing DDPMs and subsequently use them as priors
for the reconstruction of undersampled k-space. This means that the
model is not conditioned on the reconstruction tass at training time,
but at inference time. The advantage of this approach is that the model
can be used for a variety of image reconstruction tasks and is not
limited to the use case of undersampled MRI. Further, for the case of
reconstructing undersampled MRI, the reconstruction is not reliant on a
set of undersampling masks, known at training time. Thanks to the high
interpretability of DDPMs, they allow for different approaches to this
conditioning, which are explored in this work. A further focus lies on
the exploration of sampling techniques that might give better
reconstruction quality by making use of higher computational resources.

Thesis Organization
-------------------

In the first part of the thesis, the theoretical framework behind DDPMs
is established and related work is introduced, that successfully managed
to condition unconditionally-trained DDPMs. In the second part, the
conditioning methods are adapted to fit the task of reconstructing
undersampled MRI and further, the used model architectures, training
protocols and datasets are introduced. The third part shows the
experimental results from model training and model condition, and
compares the performance between the different conditioning methods, by
evaluating them over different accelerations and sampling strategies.

Materials and Methods
=====================

Loss Guided Diffusion
---------------------

Both, Choi et al. and Lugmayr et al. make use of unconditional DDPMs for
image-guided diffusion for the tasks of image translation in the former
and in-painting in the
latter. :raw-latex:`\autocite{choi2021ilvr,lugmayr2022repaint}`
Similarly, classifier guidance or CLIP-guidance can be used to condition
unconditional DDPMs to produce samples of a specific class or to match a
prompt. :raw-latex:`\autocite{dhariwal2021diffusion}` Both approaches
can be combined into a flexible framework that allows the reverse
diffusion process to be conditioned on any data consistency term.

MAP Estimation for Inverse Problems
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Image reconstruction tasks, that make use of a prior over the desired
reconstructed image, can be formulated as a MAP (maximum-a-posteriori)
estimation problem

.. math:: \hat{x}_{MAP} = \argmax_x p(x|s) = \argmax_x \frac{p(s|x)p(x)}{p(s)}

where :math:`s \sim p(s)` is the evidence that is provided by the
measured signal, :math:`p(x)` is a prior on the desired reconstruction
and the likelihood term :math:`p(s|x)` enforces a data consistency
between the measured signal and the true distribution. Since maximizing
:math:`p(x|s)` is the same as maximizing :math:`\log(x|s)` and
:math:`p(s)` is independent of :math:`\theta`, we can separate the
product into a sum.

.. math::

   \begin{aligned}
       \hat{x}_{MAP} & = \argmax_x \log p(x|s)             \\ & = \argmax_x \log{\frac{p(s|x)p(x)}{p(s)}} \\
                     & = \argmax_x \log p(s|x)p(x)         \\
                     & = \argmax_x \log p(s|x) + \log p(x)\end{aligned}

Such problems can be optimized using iterative optimization schemes like
gradient ascent

.. math::

   \begin{aligned}
       \label{eq:mapestimation}
       x_{i+1} = x_{i} + \lambda_1 \nabla_{x_i} \log p(s|x_i) + \lambda_2 \nabla_{x_i} \log p(x_i)\end{aligned}

with :math:`\lambda_{1,2}` being step sizes or weights assigned to the
two terms allowing the reconstruction to put more trust in either the
data consistency or the prior.

Maximizing :math:`p(s|x)` and :math:`p(x)` is in practice usually
reformulated as a minimization problem that optimizes for smallest error
between prediction and acquisition :math:`\mathcal{L}(s, x)` and
enforces certain regularizers (priors) on :math:`x` by minimizing
:math:`\mathcal{R}(x)`. An example for MRI reconstruction could include
minimizing a mean-squared-error between predicted k-space
:math:`\mathcal{F}(x)` and acquired k-space :math:`s`, while also
minimizing the total variation in the
image. :raw-latex:`\autocite{RUDIN1992259}`

.. math:: \hat{x}_{MAP} = \argmin_x \mathcal{L}(s, x) + \mathcal{R}(x) = \argmin_x \frac{1}{n}||\mathcal{F}(x) - s||_2^2 + TV(x)

.. _sec:lossguidance:

DDPMs as Priors
~~~~~~~~~~~~~~~

DDPMs approximate a data distribution over training images :math:`p(x)`
and by the score-based formulation, they do so by learning to
approximate gradients of this marginal
likelihood. :raw-latex:`\autocite{song2020generative}` As already
mentioned in `9.3.1 <#sec:classifierguidance>`__, sampling a DDPM
therefore equals to starting in a random position and taking gradient
ascent steps in the direction of maximizing :math:`p(x)` or
:math:`\log p(x)`.

.. math::

   \label{eq:ddpmiteration}
       x_{t-1} = x_{t} + \nabla_{x_t} \log p(x_t)

Comparing this to Eq. `[eq:mapestimation] <#eq:mapestimation>`__ it is
easy to see that this is the same as maximizing for a prior in a
reconstruction task and we can introduce a data consistency that works
similarly to
classifier-guidance :raw-latex:`\autocite{dhariwal2021diffusion}`, but
makes the reverse diffusion process converge to acquired data instead of
an easily-classified image.

.. math::

   \begin{aligned}
       \hat{x} & = \argmax_x \log p(x) + \log p_{\theta}(c|x)        &  & \text{(classifier-guidance)}       \\
               & = \argmax_x \log p(x) + \log p(s|x)                 &  & \text{(data-consistency guidance)} \\
               & = \argmax_x \log p(x) + \argmin_x \mathcal{L}(s, x) &  & \text{(for $\mathcal{L} \geq 0$)}  \\\end{aligned}

The constraint :math:`\mathcal{L} \geq 0` is true for the usual
distance-based loss functions like mean-squared-error or the :math:`L_1`
loss. A step in the iterative process has the following form and this
algorithm will from now on be termed *loss-guidance*.

.. math:: x_{t-1} = x_{t} + \nabla_{x_t} \log p(x_t) - \nabla_{x_t} \mathcal{L}(s, x)

The formulation used for the task of reconstructing undersampled MRI
used an MSE loss between the predicted :math:`\mathcal{F}(x_t)` and
acquired k-space :math:`s_0`

.. math:: x_{t-1} = x_{t} + \nabla_{x_t} \log p(x_t) - g \cdot \nabla_{x_t} \frac{1}{\sum_n \mathcal{M}}||\mathcal{M} \circ \mathcal{F}(x_t) - s_0||_2^2

where :math:`g` will be termed the *guidance factor* and the MSE is not
scaled by the number of pixels in the image, but by the number of
non-zero elements of the mask. This is done in order to compare guidance
factors among masks with different accelerations. Similar to
:math:`\lambda_{1,2}` in Eq. `[eq:mapestimation] <#eq:mapestimation>`__,
the guidance factor :math:`g` can be used to balance adherence of the
outcome between prior and data consistency.

.. _sec:freqreplacement:

Frequency Replacement
---------------------

As already stated in `9.3.2 <#sec:imageguidance>`__, Choi et al. guide
the diffusion process by substituting low frequency content of a desired
latent representation with the low-frequencies of the predicted latent
space. Since they use linear filters, the equation can be reformulated
as

.. math::

   \begin{aligned}
       \label{eq:ilvr}
       x_{t} & = \phi(s_{t}) + (I - \phi) (\hat{x}_{t})        \\
             & = \hat{x}_{t} + \phi(s_{t}) - \phi(\hat{x}_{t}) \\
             & = \hat{x}_{t} + \phi(s_{t} - \hat{x}_{t})       \\
             & = \hat{x}_{t} - \phi(\hat{x}_{t} - s_{t})\end{aligned}

where :math:`\phi` is a linear filter operation and :math:`s_t` is
obtained by using the forward process on the target
image. :raw-latex:`\autocite{choi2021ilvr}` With knowledge of the
gradient of the MSE

.. math::

   \begin{aligned}
       \text{MSE}              & = \frac{1}{N} (x - s)^T (x - s) \\
       \nabla_{x_t} \text{MSE} & = \frac{2}{N} (x - s)\end{aligned}

the frequency replacement can actually be interpreted as locally
approximating the gradient of the
:math:`\nabla_{x}\text{MSE}(\phi(s), \phi(x_{t}))` in :math:`s=s_t` and
taking a step in that direction, which is very similar to the loss
guidance from earlier.

Similarly, Lugmayr et al. use a replacement strategy, which can be
reformulated to the structure from Choi et al.

.. math::

   \begin{aligned}
       \label{eq:repaint}
       x_{t} & = \mathcal{M}(s_t) + \mathcal{M}^{-1}(\hat{x}_t)        \\
             & = \mathcal{M}(s_t) + (I - \mathcal{M})(\hat{x}_t)       \\
             & = \hat{x}_t - \mathcal{M}(\hat{x}_t) + \mathcal{M}(s_t) \\
             & = \hat{x}_t + \mathcal{M}(s_t - \hat{x}_t)\end{aligned}

As one can see, the two approaches only differ in the type of linear
operation applied, and are therefore easily adapted to the task of MRI
reconstruction. This requires calculating :math:`s_t` from :math:`s_0`
which can be done in image space as

.. math:: s_t = \mathcal{F}^{-1} \circ \mathcal{M} \circ \mathcal{F} (\sqrt{\bar{\alpha}_t} \mathcal{F}^{-1}(s_0) + \sqrt{1-\bar{\alpha}_t} \epsilon)

or directly in k-space as

.. math::

   \label{eq:kspaceforward}
       s_t = \mathcal{F}^{-1} \circ \mathcal{M} (\sqrt{\bar{\alpha}_t} s_0 + \sqrt{\frac{1-\bar{\alpha}_t}{2}} \epsilon).

The scaling of the noise variance with factor :math:`\frac{1}{2}` in
Eq. `[eq:kspaceforward] <#eq:kspaceforward>`__ was experimentally found
and can be verified in Fig. `3.3 <#fig:kspacedistribution>`__. The
complete formulation of the update step is therefore

.. math::

   \begin{aligned}
       x_{t} & = \hat{x}_t - \mathcal{F}^{-1}\circ\mathcal{M}\circ\mathcal{F}(\hat{x}_t) + \mathcal{F}^{-1} \circ \mathcal{M} \circ \mathcal{F} (s_t) \\
             & = \hat{x}_t + \mathcal{F}^{-1}\left(\mathcal{M} \circ \mathcal{F} (s_t) - \mathcal{M}\circ\mathcal{F}(\hat{x}_t)\right).\end{aligned}

.. _sec:networkarch:

Network Architecture
--------------------

The neural network is responsible for predicting the noise in an image
and the UNet architecture has proven useful for estimating the noise in
natural images, which is the context where DDPMs usually
operate. :raw-latex:`\autocite{ronneberger2015unet,ho2020denoising}` The
UNet implementation which was used in most experiments of this work is
closely related to the original implementation by Ronneberger et al.,
which means that it is a fully convolutional architecture. This is in
contrast to most other works, that make use of more sophisticated
architectures that include Transformer-inspired self-attention layers
for better global context awareness of the model and residual
connections for faster
convergence. :raw-latex:`\autocite{vaswani2017attention,he2015deep}`
Saharia et al. did ablation studies on the self-attention layers and
tried to replace them with other methods, such as local self-attention
or dilated convolutions, but showed that the global self-attention
increased both, mode coverage of the data distribution as well as sample
fidelity. :raw-latex:`\autocite{saharia2022palette}` Fully convolutional
architectures on the other hand have the advantage that, if trained
appropriately, they can generalize to different image resolutions, which
was the motivation behind using a fully convolutional network. Such
training could be done on random crops of the training images, while
sampling would happen in the full resolution. The created network was
later modified to allow for the inclusion of self-attention layers, but
the additional computational cost made it difficult to reach convergence
in a reasonable time and the results from the fully convolutional
architectures were deemed sufficient for the context of this work.
Therefore the best network checkpoint, that was used in the conditioning
studies of `6 <#sec:experimentsandresults>`__, uses the fully
convolutional architecture as presented in Fig. `8.1 <#fig:unetconv>`__.
This architecture sequentially increases the channels and decreases the
resolution with a factor of 2 in the encoder and then upsamples the
outputs of this bottleneck by incorporating additional local information
through the use of skip-connections. Every block of the encoder does
this by double-convolutions (without residual connections) and
max-pooling, while the decoder uses transpose convolutions and
double-convolutions for the upsampling. The network offers two
possibilities of increasing the total amount of parameters: 1.
Increasing the encoder depth, which is limited by the resolution of the
training images; 2. Increasing the number of initial channels, which can
for example be 64, 128 or 256. The exact network specifications can be
found in Table `8.1 <#tab:unetlayers>`__.

Noise prediction is easier for the network if it is conditioned on the
timestep of the training image. This conditioning is done by
broadcasting a linear embedding of the Transformer-style time encoding
(see Fig. `3.5 <#fig:timeencoding>`__) onto the feature dimension
(channels). :raw-latex:`\autocite{vaswani2017attention}`

.. figure:: images/unet.png
   :alt: Fully Convolutional UNet architecture: An input convolution
   increases the channels to the number of base channels *ch*.
   Consecutively a variable number of encoder blocks applies double
   convolutions and max pooling, before the decoder block applies
   transpose convolutions for upsampling and double convolutions to
   incorporate information from the skip connections. All double
   convolutions in the encoder and decoder further condition the model
   on the time encoding by broadcasting a time embedding onto the hidden
   layer, as illustrated in Fig. `3.5 <#fig:timeencoding>`__. At the
   end, the output convolution maps the output back to the original
   number of input channels.
   :name: fig:unetconv

   Fully Convolutional UNet architecture: An input convolution increases
   the channels to the number of base channels *ch*. Consecutively a
   variable number of encoder blocks applies double convolutions and max
   pooling, before the decoder block applies transpose convolutions for
   upsampling and double convolutions to incorporate information from
   the skip connections. All double convolutions in the encoder and
   decoder further condition the model on the time encoding by
   broadcasting a time embedding onto the hidden layer, as illustrated
   in Fig. `3.5 <#fig:timeencoding>`__. At the end, the output
   convolution maps the output back to the original number of input
   channels.

.. container::
   :name: tab:unetlayers

   .. table:: Overview over UNet Architecture.

      +-------------------------------+-------------------------------------+
      | **architecture part**         | **specification**                   |
      +===============================+=====================================+
      | base channels (:math:`ch`)    | :math:`2^n`, usually 64, 128, 256   |
      +-------------------------------+-------------------------------------+
      | base resolution (:math:`res`) | :math:`2^x \times 2^m`, usually     |
      |                               | square :math:`64\times 64` or       |
      |                               | :math:`128\times 128`               |
      +-------------------------------+-------------------------------------+
      | convolutional block           | convolution                         |
      +-------------------------------+-------------------------------------+
      |                               | batchnorm                           |
      +-------------------------------+-------------------------------------+
      |                               | activation                          |
      +-------------------------------+-------------------------------------+
      |                               | dropout                             |
      +-------------------------------+-------------------------------------+
      | encoder block                 | convolutional block                 |
      |                               | (:math:`ch \rightarrow ch\times 2`) |
      +-------------------------------+-------------------------------------+
      |                               | convolutional block                 |
      |                               | (:math:`                            |
      |                               | ch\times 2 \rightarrow ch\times 2`) |
      +-------------------------------+-------------------------------------+
      |                               | max pool                            |
      |                               | (:math:`res\rightarrow res/ 2`)     |
      +-------------------------------+-------------------------------------+
      | decoder block                 | transpose convolution               |
      |                               | (                                   |
      |                               | :math:`res\times 2\rightarrow res`, |
      |                               | :math:`ch\times 2 \rightarrow ch`)  |
      +-------------------------------+-------------------------------------+
      |                               | batchnorm                           |
      +-------------------------------+-------------------------------------+
      |                               | activation                          |
      +-------------------------------+-------------------------------------+
      |                               | dropout                             |
      +-------------------------------+-------------------------------------+
      |                               | skip connection stack               |
      |                               | (:math:`ch \rightarrow ch\times 2`) |
      +-------------------------------+-------------------------------------+
      |                               | convolutional block                 |
      |                               | (:math:`ch\times 2 \rightarrow ch`) |
      +-------------------------------+-------------------------------------+
      | bottleneck                    | convolutional block                 |
      |                               | (:math:`ch \rightarrow ch\times 2`) |
      +-------------------------------+-------------------------------------+

.. _sec:morecompute:

Slowing Down, Short-Grained Resampling & Long-Grained Resampling
----------------------------------------------------------------

Various approaches exist to give the reverse diffusion process more time
to converge to a meaningful final prediction. The already introduced
resampling used by Lugmayr et al. will be termed *short-grained
resampling*, since it resamples predictions in small temporal
environments, called the jump length, before continuing the reverse
diffusion. *Long-grained resampling* on the other hand finishes the
reverse diffusion in every single resampling, but restarts the process
at progressively decreasing timesteps. Plots of the two schedules can be
compared in Fig. `3.4 <#fig:stepsplot>`__.

Instead of resampling it is also possible to decrease the step size of
the sampling process and use more steps to reach the final prediction.
Apart from the reverse variances, this also requires the time encoding
to be adapted in order to represent the intermediate steps. Similarly,
the sampling process could also be accelerated by increasing the step
size and using less steps in total.

Datasets
--------

Introductory experiments were conducted on low-resolution datasets in
order to debug the model implementation and determine the best training
strategies. These datasets were the well-known MNIST and
CIFAR10 :raw-latex:`\autocite{mnist,cifar}` in resolutions of
:math:`28\times28` and :math:`32\times32` pixels respectively. Since the
encoder stack relies on image resolutions of :math:`2^n\times2^k` with
:math:`n,k\in \mathbb{N}`, the MNIST images were upscaled to an equal
:math:`32\times32` resolution. The MNIST dataset is a dataset containing
60’000 training images of handwritten digits 0 to 9 and CIFAR10 contains
50’000 training images distributed over 10 classes like airplane, bird,
cat, etc.

The main dataset used in the experiments were the RSS (root sum of
squares) reconstructions from the brain dataset in
fastMRI. :raw-latex:`\autocite{zbontar2018fastMRI}` FastMRI is a
collection of several MRI datasets, a large dataset of multi-coil brain
scans among them. In addition to the raw multi-coil data, RSS
reconstructions, combining the coils by using estimates of the
sensitivity maps, are also available. Those reconstructions have very
high quality and therefore provide a strong basis for useage as a prior
in the reconstruction task. The RSS reconstructions contain a total of
60’090 slices of resolution :math:`320\times320` pixels and models were
trained on downsampled versions of :math:`256\times 256`,
:math:`128\times 128` and :math:`64\times 64` pixels.

While the authors of fastMRI suggest equally-spaced masks with a fixed
center fraction for brain
images :raw-latex:`\autocite{zbontar2018fastMRI}`, the masks used in
this work have fixed center fractions, but are randomly sampled for the
higher frequencies. Three masks, and the effect they have on the
samples, are shown in Fig. `8.2 <#fig:kspacemasking>`__. These masks are
similar to the ones used in the experimental part.

.. figure:: images/corruption_mask.png
   :alt: K-Space Undersampling: a) Sample of a K-Space undersampling
   mask with center fraction of 0.1 and a probability of 0.25 for the
   other frequencies, giving an effective acceleration of
   :math:`\approx 3.12`. b) Effect of K-Space undersampling on samples
   from fastMRI dataset. While the low-frequency content is visibly
   intact, the undersampling of the higher frequencies causes aliasing
   artifacts, manifested as lines on the samples. These lines hint at
   the true aliases that would be generated for image space
   undersampling instead of randomized k-space undersampling.
   :name: fig:kspacemasking

   K-Space Undersampling: a) Sample of a K-Space undersampling mask with
   center fraction of 0.1 and a probability of 0.25 for the other
   frequencies, giving an effective acceleration of
   :math:`\approx 3.12`. b) Effect of K-Space undersampling on samples
   from fastMRI dataset. While the low-frequency content is visibly
   intact, the undersampling of the higher frequencies causes aliasing
   artifacts, manifested as lines on the samples. These lines hint at
   the true aliases that would be generated for image space
   undersampling instead of randomized k-space undersampling.

Software Package
----------------

In order to fully understand DDPMs it was decided to implement them from
scratch instead of using repositories provided by the
literature :raw-latex:`\autocite{nichol2021improved}` or by packages
such as the Huggingface Diffusers
library. :raw-latex:`\autocite{huggingfacediffusers}` The created
repository is publicly accessible via GitHub and includes automatic
documentation generation using Sphinx :raw-latex:`\autocite{sphinx}` and
GitHub Actions :raw-latex:`\autocite{githubactions}`. Notable is also
the useage of jaxtyping :raw-latex:`\autocite{jaxtyping}`, a library for
type hinting tensor shapes. The repository and the documentation are
accessible under the following links:

| ```https://github.com/liopeer/diffusionmodels`` <#https://github.com/liopeer/diffusionmodels>`__
| ```https://liopeer.github.io/diffusionmodels/`` <#https://liopeer.github.io/diffusionmodels/>`__

The software package is based on
PyTorch :raw-latex:`\autocite{paszke2019pytorch}` and provides model
architectures as well as training utilities. These utilities include the
possibility for 1. distributed training, 2. training logging and
checkpointing, 3. mixed-precision training and inference, implemented
using the following frameworks.

Weights & Biases
   provides an API that allows logging the model training via their
   website (```https://wandb.ai/`` <#https://wandb.ai/>`__). The tool is
   free for students and academic researchers and automatically logs
   model configuration, gradients and hardware parameters in addition to
   user-specified logs, such as sample images, losses and inference
   times. When using git for versioning it also logs the most recent git
   commit, allowing to resume model training or to rerun an experiment
   with exactly the same code. When training models over several days it
   was very convenient to be able to observe the process from the
   smartphone and look at samples generated by the
   model. :raw-latex:`\autocite{wandb}`

PyTorch DDP (DistributedDataParallel)
   parallelizes model training by launching individual processes for
   each GPU, or it can even launch processes across different machines.
   Separate processes are necessary in order to enable true parallelism
   that avoids Python GIL (global interpreter lock). During
   initialization, the model is copied across the different GPUs and
   during training only the gradients are synchronized and averaged
   across the GPUs, therefore the optimizers essentially train a local
   model per each process. Gradient synchronization is automatically
   invoked by calling ``loss.backward()``, but can be avoided by
   including forward and backward passes of the neural network in the
   ``no_sync()`` content manager, which is useful when using gradient
   accumulation over several micro-batches, where the gradient
   synchronization would create unnecessary overhead. As part of DDP,
   PyTorch also offers ``DistributedSampler`` (to be used with
   ``DataLoader``), which splits mini-batches into micro-batches and
   assigns them to the respective processes. For models that use batch
   normalization layers, DDP also offers the module ``SyncBatchNorm``
   and a function to recursively change all batch normalization layers
   to synchronized batch normalization. Synchronizing the batch
   normalization might be important for small micro-batch sizes or when
   the number of GPUs changes during training (e.g. continuing from a
   checkpoint).

PyTorch AMP (Automatic Mixed Precision)
   provides a context manager and a function decorator that will convert
   certain operations to half-precision (16 bit), which gives a
   significant speedup for linear layers or convolutions, but keeps high
   precision for operations such as reductions. Half precision training
   might lead to underflow of gradients, because of the reduced value
   range and can be avoided by scaling the loss and therefore the
   gradients, while also inversely scaling the update step. AMP provides
   the ``GradScaler`` class for this purpose.

Related Work
============

Latent Variable Models
----------------------

Latent variable models are generative models which assume that it is
possible to model the true data distribution :math:`p(x)` as a joint
distribution :math:`p(x,z)`, where :math:`x` and :math:`z` are
multi-variate random vectors.

.. math::

   \label{eq:marginallikelihood}
       p(x) = \int p(x,z)dz = \int p(x|z)p(z)dz

Many naturally occurring distributions of samples can be imagined to
come from a much simpler underlying distribution, but the distribution
is obscured by the space that the samples are observed in. This is the
main motivation behind latent variable models and in order to understand
these models, it is important to define the terms used in the next
sections, since they all stem from Bayesian statistics. The Bayesian
theorem can be written as

.. math::

   \label{eq:bayestheorem}
       p(z|x) = \frac{p(x|z)p(z)}{p(x)}

which is a generally applicable formula for any conditional
distributions, but in generative modeling and machine learning it is
usually assumed that the letter :math:`z` represents a random vector of
such a simpler distribution in the latent (unobserved) space, and
:math:`x` is the random vector modeling the complicated distribution in
the observed space (the sample space). The four terms in the formula use
distinct names:

:math:`p(x)`
   is called the *evidence* or the *marginal likelihood*. It encompasses
   the actual observations of the data.

:math:`p(z)`
   is called the *prior*, since it exposes information on :math:`z`
   before any conditioning.

:math:`p(z|x)`
   is called the *posterior*. It describes the distribution over
   :math:`z` after (*post*) having seen the evidence :math:`x`.

:math:`p(x|z)`
   is called the *likelihood*, since it gives the likelihood of
   observing an example :math:`x` when choosing the latent space to be a
   specific :math:`z`.

Variational Autoencoders
~~~~~~~~~~~~~~~~~~~~~~~~

One of the most straightforward examples of a generative model, where
the goal is to find such a latent space representation of the training
sample distribution, is the Variational Autoencoder
(VAE) :raw-latex:`\autocite{kingma2013autoencoding}`. For the two
factors in Eq. `[eq:marginallikelihood] <#eq:marginallikelihood>`__, the
VAE uses a simple multivariate distribution as the latent
:math:`p_{\theta_z}(z)` (e.g. a multivariate Gaussian) and a neural
network mapping :math:`p_{\theta_{NN}}(x|z)`. Training then includes
finding optimal parameters for the parameterized latent distribution and
for the neural network mapping, such that sampling :math:`z` and mapping
it to the sample space is almost the same as sampling :math:`x`
directly. When no prior over the parameters
:math:`\theta_z, \theta_{NN}` is considered, this is usually done
through an MLE (maximum likelihood estimate)
:math:`\hat{\theta} = \argmax_{\theta} p_{\theta}(x|\theta)`. While
simple latent variable models can be optimized directly through
differentiation, more complicated models need iterative algorithms such
as EM (expectation maximization) and gradient descent. These algorithms
usually don’t work for complicated multi-modal distributions (as
parameterized by neural networks), since the integral in
Eq. `[eq:marginallikelihood] <#eq:marginallikelihood>`__ has no closed
form solution and is also difficult or costly to estimate. Therefore it
would be preferred to use a parameterization instead that also uses an
estimation of the posterior :math:`p_{\theta}(z|x) \approx p(z|x)` as in
Bayes’ theorem, which directly leads to the VAE.

The name of the VAE stems from the Autoencoder, a neural network that
learns to recreate its input through a bottleneck, thereby learning a
compressed representation of the
data. :raw-latex:`\autocite{https://doi.org/10.1002/aic.690370209}`
Autoencoders bear similarity to other dimension reduction methods like
Principal Component Analysis (PCA) and therefore were first published
under the name *Nonlinear principal component analysis*. Responsible for
compressing and decompressing the data are two neural networks, termed
encoder and decoder. By using neural networks for posterior and
likelihood in the VAE, the architecture indeed resembles an autoencoder,
as illustrated in Fig. `9.1 <#fig:vae>`__.

.. figure:: images/vae.png
   :alt: VAE schematic: :math:`p(x)` is approximated through a latent
   variable model where posterior and likelihood are modeled with neural
   networks and the prior on the latent variable is modeled through a
   simple parameterized distribution (often Gaussian). The hope is, that
   after training, sampling from :math:`p(z)` and passing it through the
   neural network :math:`p_{\theta_{NN}}(x|z)`, is the same as sampling
   from :math:`p(x)`.
   :name: fig:vae

   VAE schematic: :math:`p(x)` is approximated through a latent variable
   model where posterior and likelihood are modeled with neural networks
   and the prior on the latent variable is modeled through a simple
   parameterized distribution (often Gaussian). The hope is, that after
   training, sampling from :math:`p(z)` and passing it through the
   neural network :math:`p_{\theta_{NN}}(x|z)`, is the same as sampling
   from :math:`p(x)`.

KL Divergence and Variational Lower Bound
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In VAEs, the encoder :math:`p_{\theta}(z|x)` needs to approximate the
true posterior :math:`p(z|x)` and sampled data should look like it was
sampled from :math:`p(x)`. This requires a measure that can compare the
similiarity between two probability distributions. One such heavily used
measure is the KL (Kullback-Leibler) divergence, formulated for the
posterior and its approximation as

.. math::

   \label{eq:kldivergence}
       KL\left[p_{\theta}(z|x) || p(z|x)\right] = \int \log \frac{p_{\theta}(z|x)}{p(z|x)} p_{\theta}(z|x) dz = \mathbb{E}_{z\sim p_{\theta}(z|x)}\left[\log \frac{p_{\theta}(z|x)}{p(z|x)}\right].

The KL divergence has the properties of being strictly non-negative and
only being 0 if the two distributions are equal, but the proofs of those
properties are omitted in this work.

The problem with the KL divergence in
Eq. `[eq:kldivergence] <#eq:kldivergence>`__ is that the true posterior
is unknown. We will therefore introduce a loss function called ELBO
(evidence lower bound) or VLB (variational lower bound) that
automatically makes sure that the KL divergence between the
parameterized posterior and the true posterior is minimized, without
knowing :math:`p(z|x)`. For understanding the ELBO, it is important to
note that the marginal log-likelihood can be written as follows
(derivation in the appendix):

.. math::

   \begin{aligned}
       \log p_{\theta}(x) & = \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log \frac{p_{\theta_{NN}}(x|z) p_{\theta_z}(z)}{p_{\theta_{NN}}(z|x)}\right] + KL\left[p_{\theta_{NN}}(z|x)||p(z|x)\right]\end{aligned}

From the properties of the KL divergence we know that the second term on
the right hand side is strictly non-negative, this means that the first
term on the right hand side offers a lower bound to the log-likelihood
of the data

.. math::

   \label{eq:elbo}
       \log p_{\theta}(x) \geq \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log\frac{p_{\theta_{NN}}(x|z) p_{\theta_z}(z)}{p_{\theta_{NN}}(z|x)}\right]

and the difference between that first term and the log-likelihood of the
data is exactly the KL divergence that we wanted to minimize in
Eq. `[eq:kldivergence] <#eq:kldivergence>`__. The relationship is also
illustrated in Fig. `9.2 <#fig:elbo>`__.

.. figure:: images/elbo.png
   :alt: Illustration of the ELBO/VLB: The ELBO term gives a lower bound
   to the log-likelihood since the KL divergence is strictly
   non-negative. By maximizing the ELBO term, the KL divergence term is
   implicitly minimized and ELBO term converges towards the
   log-likelihood.
   :name: fig:elbo

   Illustration of the ELBO/VLB: The ELBO term gives a lower bound to
   the log-likelihood since the KL divergence is strictly non-negative.
   By maximizing the ELBO term, the KL divergence term is implicitly
   minimized and ELBO term converges towards the log-likelihood.

Therefore, if we could maximize the ELBO term from
Eq. `[eq:elbo] <#eq:elbo>`__, it would not only approach the
log-likelihood, but simultaneously make sure that the estimated
posterior converges to the true posterior. Luckily, for the
parameterization of the VAE, the ELBO term can be split into two
interpretable parts for optimization.

.. math::

   \begin{aligned}
       \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log\frac{p_{\theta_{NN}}(x|z) p_{\theta_z}(z)}{p_{\theta_{NN}}(z|x)}\right] & = \mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log p_{\theta_{NN}}(x|z)\right] - \mathbb{E}_{z\sim p_{\theta_{NN}}(x|z)}\left[\log \frac{p_{\theta_{NN}}(z|x)}{p_{\theta_{z}}(z)}\right]                                        \\
                                                                                                                                 & = \underbrace{\mathbb{E}_{z\sim p_{\theta_{NN}}(z|x)}\left[\log p_{\theta_{NN}}(x|z)\right]}_{\text{reasonable reconstruction}} - \underbrace{KL \left[p_{\theta_{NN}}(z|x)||p_{\theta_{z}}(z)\right]}_{\text{correct encoding}}\end{aligned}

Maximizing the first part makes sure that the decoder reconstructs
reasonable samples from the latent distribution, minimizing the second
makes sure that the encoder transforms the training data into our chosen
prior over the latents :math:`z` (usually Gaussian, as mentioned
before). The reconstruction term is trivially maximized by minimizing
some loss between input and output and if the prior :math:`p(z)` is
chosen to be a Gaussian :math:`p_{\theta_{z}}(z)`, the KL divergence has
a closed form, the derivation of which is
omitted. :raw-latex:`\autocite{mreasykldivergence}`

.. math:: D_{KL}(p||q) = \frac{1}{2}\left[\log\frac{|\Sigma_q|}{|\Sigma_p|} - k + (\boldsymbol{\mu_p}-\boldsymbol{\mu_q})^T\Sigma_q^{-1}(\boldsymbol{\mu_p}-\boldsymbol{\mu_q}) + tr\left\{\Sigma_q^{-1}\Sigma_p\right\}\right]

In the next section, the DDPM (diffusion denoising probabilistic model)
will be introduced, which is the model architecture used throughout this
work. As will be clear shortly, DDPMs can be viewed as a chained VAE
that uses a sequence of latent spaces. This is an arguably easier
learning problem, since the neural network does not have to map directly
from noise to samples, but can do so in an iterative process over many
steps.

Diffusion Denoising Probabilistic Models
----------------------------------------

Diffusion Denoising Probabilistic Models (DDPMs or Diffusion Models) are
a generative model that are especially suited to learning the
distribution of images in a training set. During training, sample images
are gradually destroyed by adding noise over many iterations and a
neural network is trained such, that these steps can be inverted. This
destructive process can be interpreted as a diffusion of the information
in the image, hence their name and the interpretation of the destruction
steps as *timesteps*. At time :math:`0`, an image has not started the
diffusion process, therefore we use the random variable :math:`\bm{x}_0`
to represent original training images, :math:`\bm{x}_t` for (partially
noisy) images at an intermediate timestep and :math:`\bm{x}_T` for
images at the end of the process where all information has been
destroyed, and the distribution :math:`q(\bm{x}_T)` should follow an
isotropic Gaussian distribution.

Inverting this process means training a network that creates a less
noisy image :math:`\bm{x}_{t-1}` from :math:`\bm{x}_t`. If this is
achieved over the whole training distribution, then sampling a new
:math:`\bm{x}_T` and iteratively denoising it, should be the same as
sampling :math:`q(\bm{x}_0)` directly.

.. _sec:forwarddiffusion:

Forward Diffusion Process
~~~~~~~~~~~~~~~~~~~~~~~~~

In order to derive a training objective it is important to understand
the workings of the *forward diffusion process*. During this process,
i.i.d (independent and identically distributed) Gaussian noise is
applied to the image over many discrete timesteps. A *variance schedule*
:math:`\beta(t)` defines the variances and means (:math:`\beta_t` and
:math:`\sqrt{1-\beta_t}`) of the added noise at every
timestep. :raw-latex:`\autocite{ho2020denoising}` The whole process can
be expressed as a Markov chain (depicted in
Fig. `9.3 <#fig:forward_diffusion>`__), with the factorization

.. math::

   \begin{aligned}
       \label{eq:forwardprocess}
       q(\bm{x}_{0:T})            & = q(\bm{x}_0) \prod_{t=1}^{T} q(\bm{x}_{t}|\bm{x}_{t-1}) &  & \text{(joint distribution)}       \\
       q(\bm{x}_{0:T}|\bm{x}_{0}) & = \prod_{t=1}^{T} q(\bm{x}_{t}|\bm{x}_{t-1})             &  & \text{(forwarding single sample)}\end{aligned}

where the transition distributions
:math:`q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \bm{x}_{t-1}, \beta_t I)`
and we used the shorthand notation
:math:`\bm{x}_{0:T} = \bm{x}_{0},\dots,\bm{x}_{T}`. An example of
iterative destruction of an image by this process is shown in
Fig. `9.4 <#fig:forward_naoshima>`__.

.. figure:: images/forward_diffusion.png
   :alt: Markov Chain Interpretation of Forward Diffusion Process: An
   image is iteratively destroyed by adding normally distributed noise,
   according to a schedule. This represents a Markov process with the
   transition probability :math:`q(\bm{x}_t|\bm{x}_{t-1})`.
   :name: fig:forward_diffusion

   Markov Chain Interpretation of Forward Diffusion Process: An image is
   iteratively destroyed by adding normally distributed noise, according
   to a schedule. This represents a Markov process with the transition
   probability :math:`q(\bm{x}_t|\bm{x}_{t-1})`.

.. figure:: images/forward_naoshima_clean.png
   :alt: Example of Iterative Image Destruction through Forward
   Diffusion Process: The indices give the time step in the iterative
   destruction process, where the :math:`\beta_t` were created according
   to a linear noise variance schedule (1200 steps in the 0.001 to 0.02
   range and picture resolution of 120\ :math:`\times`\ 180 pixels).
   :name: fig:forward_naoshima

   Example of Iterative Image Destruction through Forward Diffusion
   Process: The indices give the time step in the iterative destruction
   process, where the :math:`\beta_t` were created according to a linear
   noise variance schedule (1200 steps in the 0.001 to 0.02 range and
   picture resolution of 120\ :math:`\times`\ 180 pixels).

Gladly it is not necessary to sample noise again and again in order to
arrive at :math:`\bm{x}_t`, since Ho et al. derived a closed-form
solution to the sampling
procedure. :raw-latex:`\autocite{ho2020denoising}` For this, the
variance schedule is first reparameterized as :math:`1-\beta = \alpha`,
turning the variance schedule into a schedule of the means

.. math::

   q(\bm{x}_t | \bm{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \bm{x}_{t-1}, (1-\alpha_t) \bm{I}).
       \label{eq:forward_alpha}

The closed-form solution for :math:`q(\bm{x}_t|\bm{x}_0)` is
subsequently derived by introducing the cumulative product
:math:`\bar{\alpha}_t = \prod_{s=1}^{t}\alpha_s` and following the
derivation in appendix `1.1 <#app:forward>`__.

.. math::

   q(\bm{x}_t|\bm{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t}\bm{x}_0, (1-\bar{\alpha}_t)\bm{I})
       \label{eq:forward_alphadash}

A choice of :math:`\bar{\alpha}_t \in [0,1]` in above parameterizaiton
ensures that the variance does not explode in the process, but that the
SNR (signal-to-noise-ratio) still goes to 0 by gradually attenuating the
means, corresponding to the original image. Thanks to the
reparameterization with :math:`\bar{\alpha}_t`, the forward process is
also not restricted anymore to discrete timesteps, but a continuous
schedule can be
used. :raw-latex:`\autocite{kingma2023variational,song2021scorebased}`

The process of information destruction is dependent on the chosen
variance schedule, the number of steps and the image size. Beyond the
most simple case – a constant variance over time – Ho et al. opted for
the second most simple option, a linear schedule, where the variance
:math:`\beta_t` grows linearly in
:math:`t`. :raw-latex:`\autocite{ho2020denoising}` Nichol et al. later
found that a cosine-based schedule gives better results on lower
resolution images, since it does not destruct information quite as
quickly, making it more informative in the last few timesteps. They also
mention that their cosine schedule is purely based on intuition and they
expect similar functions to perform equally
well. :raw-latex:`\autocite{nichol2021improved}` Own experiments
exploring above mentioned parameters are explained
in `6.1.1 <#sec:forward_diff_experiments>`__ and plots of the two
different variance schedules are visible in
Fig. `6.4 <#fig:alphadash>`__.

Reverse Diffusion Process
~~~~~~~~~~~~~~~~~~~~~~~~~

As mentioned before DDPMs can be viewed as latent space models in a
similar way that Generative Adversarial Nets or Variational Autoencoders
can. :raw-latex:`\autocite{goodfellow2014generative,kingma2013autoencoding}`
In DDPMs the reverse process is essentially again a Markov chain and can
therefore again be factorized as

.. math::

   \label{eq:reverseprocess}
       q(\bm{x}_{0:T}) = q(\bm{x}_T) \prod_{t=T}^{1} q(\bm{x}_{t-1}|\bm{x}_{t})

where we start from :math:`\bm{x}_T\sim\mathcal{N}(0,\bm{I})`. This
means that the network does not learn to approximate the full inversion,
but rather just the transition probabilities
:math:`q(\bm{x}_{t-1}|\bm{x}_{t})` in the chain, which are transitions
between several intermediate latent distributions. During training, the
inversion needs to be conditioned on the training samples, since this is
where we would like to arrive at the end, and this inversion is gladly
also Gaussian, as proven in the appendix.

This means that in contrast to the VAE, we have a known Gaussian
posterior :math:`p(x_{t}|x_{t-1})` from the forward process and an
inversion :math:`q(x_{t}|x_{t+1})` that is also Gaussian, making them
easy to match. This matching is done by approximating the inversion
using a neural network as

.. math::

   \label{eq:reverseapprox}
       q(\bm{x}_{t-1} | \bm{x}_t) \approx p_{\theta}(\bm{x}_{t-1} | \bm{x}_t) = \mathcal{N}(\bm{\mu}_{\theta}(\bm{x}_t, t),\bm{\Sigma}_{\theta}(\bm{x}_t, t)).

Loss Functions
~~~~~~~~~~~~~~

The combination of forward :math:`q(\bm{x}_T|\bm{x}_0)` and reverse
process :math:`q(\bm{x}_0|\bm{x}_T)` can be viewed as a chain of VAEs
and we can again formulate a variational lower bound objective like
before. The lengthy derivation of the ELBO for the DDPM is omitted in
this work, but can be looked up in the Calvin Luo’s
work. :raw-latex:`\autocite{luo2022understanding}` The final form is
similar to the one from the VAE, with a reconstruction term and a prior
matching term, but with additional terms that match the intermediate
latents.

.. math::

   \begin{aligned}
       \log p_{\theta}(x) & \geq \underbrace{\mathbb{E}_{q(x_1|x_0)} \left[ \log p_{\theta}(x_0|x_1) \right]}_{\text{reasonable reconstruction}}          \\
                          & - \underbrace{KL \left[ q(x_T|x_0) || p(x_t) \right]}_{\text{correct encoding}}                                               \\
                          & - \sum_{t=2}^{T} \underbrace{KL \left[ q(x_{t-1}|x_{t},x_0) || p_{\theta}(x_{t-1}|x_{t}) \right]}_{\text{denoising matching}}\end{aligned}

The natural choice for the denoising matching term would be
:math:`KL\left[ q(x_t|x_{t-1}) || p_{\theta}(x_t|x_{t+1}) \right]`, but
this has higher variance than above term :math:`q(x_{t-1}|x_{t},x_0)`
and is therefore harder to
estimate. :raw-latex:`\autocite{ho2020denoising}` The term
:math:`q(x_{t-1}|x_{t},x_0)` is the true reverse process, conditioned on
a single sample. This term comes to be when substituting the posterior
transitions :math:`p(x_t|x_{t-1})` with :math:`p(x_t|x_{t-1}, x_0)`,
which is allowed since the Markov property states that :math:`x_t` only
depends on :math:`x_{t-1}`. Due to the DDPM usually having 1000 or more
timesteps, the ELBO is dominated by the third term. For this reason the
first term is usually not used during optimization, but it can be useful
for evaluating the performance of a trained model, by Monte-Carlo
estimates of the log-likelihood. The second term is parameter-free
therefore also not used for optimization. It should anyway be zero if
the parameterization of the forward process is correct, which means that
forward diffused samples get close to the chosen latent prior
:math:`p(\bm{x}_T) = \mathcal{N}(0,\bm{I})`. As mentioned before,
:math:`p_{\theta}(x_{t-1}|x_t)` is also Gaussian and since it was
decided to fix the variances of the forward transitions to a fixed
schedule, the variances of the inversion are often fixed as well and
only the means are learned
:math:`p_{\theta}(\bm{x}_{t-1} | \bm{x}_t) = \mathcal{N}(\bm{\mu}_{\theta}(\bm{x}_t, t),\sigma (t) \bm{I})`.
When looking at the formula for the KL divergence between two Gaussians
(Eq. `[eq:kldivergence] <#eq:kldivergence>`__) with diagonal covariance
matrices :math:`\bm{\Sigma} = k*\bm{I}` for some constant :math:`k`, one
can derive that it reduces to a mean squared error between the
distributional means. :raw-latex:`\autocite{luo2022understanding}`

.. math:: \hat{\theta} = \argmin_{\theta} KL \left[ q(x_{t-1}|x_{t},x_0) || p_{\theta}(x_{t-1}|x_{t}) \right] = \argmin_{\theta} \frac{1}{2\beta(t)^2} \left[ || \mu_{\theta} - \mu_{q} ||_2^2 \right]

Ho et al. :raw-latex:`\autocite{ho2020denoising}` further found that it
works best, if the network is trained to predict the noise in the image
directly and the means are then found through reparameterization

.. math:: \mu_{\theta}(x_t,t) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\hat{\epsilon}_{\theta}(x_t,t)

which transforms the loss from before
into :raw-latex:`\autocite{luo2022understanding}`

.. math:: \hat{\theta} = \argmin_{\theta}\frac{1}{2\beta(t)^2} \frac{(1-\alpha_t)^2}{(1-\bar{\alpha}_t)\alpha_t} \left[ || \epsilon_0 - \hat{\epsilon}(x_t, t)  ||_2^2 \right].

Guided Diffusion
----------------

DDPMs have another interpretation as energy-based models, where the
inverse transitions are seen as local gradient approximations of the
training data density. Sampling from a DDPM is therefore equal to
starting at a random point and taking gradient ascent steps along the
direction of the steepest ascent in order to eventually land in a
distributional mode, where the sample likelihood is maximized. This
corresponds to the following inversion update rule:

.. math::

   \label{eq:updatescore}
       x_{t-1} = x_{t} + \nabla_{x_t} \log p(x_t)

One way of conditioning DDPMs at inference is by altering the update
step in Eq. `[eq:updatescore] <#eq:updatescore>`__, which is usually
done by introducing additional gradients in order to make the model
converge towards a desired distributional mode.

A different method of conditioning is derived from the known posterior
:math:`p(x_t|x_0)`, which allows to inject information about some target
image into the latent representations. Information about a target image
:math:`s_0` can be forward diffused like :math:`x_0` during training,
and the current prediction :math:`x_t` can be merged with :math:`s_t` .
The next two sections introduce three successfully used conditioning
methods that make use of above interpretations.

.. _sec:classifierguidance:

Classifier Guidance
~~~~~~~~~~~~~~~~~~~

Classifier guidance as termed by Dhariwal et al. introduces a data
consistency term :math:`p(s|x_t)` in the form of a classifier trained on
noisy images, where :math:`s` is the random variable expressing if an
image belongs to a certain
class. :raw-latex:`\autocite{dhariwal2021diffusion,sohldickstein2015deep}`
Conditioning on a classifier is sucessfully used by taking gradient
ascent steps not only in the direction that maximizes the log-likelihood
in a DDPM, but also the direction of this conditioning term
:math:`\nabla_{x_t} \log p(s|x_t)`.

.. math:: x_{t-1} = \underbrace{x_{t} + \nabla_{x_t} \log p(x_t)}_{x'_{t+1}} + \lambda \nabla_{x_t} \log p(s|x_t)

The gradient ascent is therefore trying to maximize the confidence of
the classifier, which not only conditions the model to produce samples
of a certain class, but enhances the sample fidelity by producing easily
classifiable samples. :raw-latex:`\autocite{dhariwal2021diffusion}`
Instead of a simple classifier that conditions on an image class, it is
also possible to use more sophisticated models like
CLIP :raw-latex:`\autocite{radford2021learning}` that condition on a
match between a text prompt and a latent prediction. It should be noted
that these models need to be trained on noisy DDPM latents, models
trained on ground truth images will fail on the conditioning.

.. _sec:imageguidance:

Image-Guided Diffusion
~~~~~~~~~~~~~~~~~~~~~~

Lugmayr et al. make use of the known posterior for the tasks of image
inpainting by substituting known parts of the latent image areas during
the reverse diffusion :raw-latex:`\autocite{lugmayr2022repaint}`

.. math:: x_{t} = \mathcal{M}^{-1}(x_t) + \mathcal{M}(s_t)

with :math:`\mathcal{M}` being a mask and :math:`s_t` the latent
representation of the known image parts. They further enhance their
approach using a resampling strategy, that gives the model more time to
harmonize the semantics of the image. An example of such a resampling
schedule can be seen in Fig. `3.4 <#fig:stepsplot>`__.

Choi et al. also substitute parts of the image in order to guide the
inverse diffusion process, but they substitute low-frequency information
by using linear filters. :raw-latex:`\autocite{choi2021ilvr}`

.. math:: x_{t} = \phi(s_{t}) + (I - \phi) (x_{t})

They demonstrate strong performance in image translation tasks, e.g.
from painting to photo-realistic image.

**Conditioning of DDPMs on Accelerated MRI**
Semester Thesis
Lionel Peer
Department of Information Technology and Electrical Engineering

+-----------------+---------------------------------------------------+
| **Advisors:**   | Georg Brunner & Emiljo Mëhillaj                   |
+-----------------+---------------------------------------------------+
| **Supervisor:** | Prof. Dr. Ender Konukoglu                         |
+-----------------+---------------------------------------------------+
|                 | Computer Vision Laboratory, Group for Biomedical  |
|                 | Image Computing                                   |
+-----------------+---------------------------------------------------+
|                 | Department of Information Technology and          |
|                 | Electrical Engineering                            |
+-----------------+---------------------------------------------------+
