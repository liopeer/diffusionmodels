<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract &mdash; DiffusionMRI 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=d45e8c67"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Idea Corner" href="../idea_corner.html" />
    <link rel="prev" title="diffusion_models.utils.mp_setup.DDP_Proc_Group" href="../_autosummary/diffusion_models.utils.mp_setup.DDP_Proc_Group.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/diffMRI_logo.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Homepage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="#acknowledgements">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-first-appendix">The First Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="#extended-derivations">Extended Derivations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#forward-process-closed-form">Forward Process Closed-Form</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="#discussion">Discussion</a></li>
<li class="toctree-l1"><a class="reference internal" href="#experiments-and-results">Experiments and Results</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#influence-of-schedules-and-image-size-on-the-forward-diffusion">Influence of Schedules and Image Size on the Forward Diffusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#focus-of-this-work">Focus of this Work</a></li>
<li class="toctree-l2"><a class="reference internal" href="#thesis-organization">Thesis Organization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#materials-and-methods">Materials and Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="#generative-machine-learning">Generative Machine Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#bayesian-formulation-of-latent-variable-models">Bayesian Formulation of Latent Variable Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#variational-autoencoders">Variational Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#diffusion-denoising-probabilistic-models">Diffusion Denoising Probabilistic Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#forward-diffusion-process">Forward Diffusion Process</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mathematical-description">Mathematical Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#influence-of-scheduling-functions">Influence of Scheduling Functions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#reverse-diffusion-process">Reverse Diffusion Process</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#latent-variable-models-compared">Latent Variable Models Compared</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss-functions">Loss Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#kullback-leibler-divergence">Kullback-Leibler Divergence</a></li>
<li class="toctree-l3"><a class="reference internal" href="#wasserstein-distance">Wasserstein Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#variational-lower-bound">Variational Lower Bound</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#related-work">Related Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="../idea_corner.html">Idea Corner</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DiffusionMRI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Abstract</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/chap/tex_stuff.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="abstract">
<h1>Abstract<a class="headerlink" href="#abstract" title="Link to this heading"></a></h1>
<p>The abstract gives a concise overview of the work you have done. The
reader shall be able to decide whether the work which has been done is
interesting for him by reading the abstract. Provide a brief account on
the following questions:</p>
<ul class="simple">
<li><p>What is the problem you worked on? (Introduction)</p></li>
<li><p>How did you tackle the problem? (Materials and Methods)</p></li>
<li><p>What were your results and findings? (Results)</p></li>
<li><p>Why are your findings significant? (Conclusion)</p></li>
</ul>
<p>The abstract should approximately cover half of a page, and does
generally not contain citations.</p>
</section>
<section id="acknowledgements">
<h1>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Link to this heading"></a></h1>
</section>
<section id="the-first-appendix">
<h1>The First Appendix<a class="headerlink" href="#the-first-appendix" title="Link to this heading"></a></h1>
<p>In the appendix, list the following material:</p>
<ul class="simple">
<li><p>Data (evaluation tables, graphs etc.)</p></li>
<li><p>Program code</p></li>
<li><p>Further material</p></li>
</ul>
</section>
<section id="extended-derivations">
<h1>Extended Derivations<a class="headerlink" href="#extended-derivations" title="Link to this heading"></a></h1>
<section id="forward-process-closed-form">
<span id="app-forward"></span><h2>Forward Process Closed-Form<a class="headerlink" href="#forward-process-closed-form" title="Link to this heading"></a></h2>
<p>Starting with transition distributions</p>
<div class="math notranslate nohighlight">
\[q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \bm{x}_{t-1}, \beta_t I)\]</div>
<p>the reparameterization <span class="math notranslate nohighlight">\(\alpha = 1 - \beta\)</span> is introduced</p>
<div class="math notranslate nohighlight">
\[q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \bm{x}_{t-1}, (1-\alpha) I)\]</div>
<p>which can also be formulated as</p>
<div class="math notranslate nohighlight">
\[q(\bm{x}_t|\bm{x}_{t-1}) = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t}\mathcal{N}(\bm{0}, \bm{I}).\]</div>
<p>For coherent indexing it is beneficial to switch to notation using
random variables</p>
<div class="math notranslate nohighlight">
\[\bm{x}_{t} = \sqrt{\alpha_t}\bm{x}_{t-1} + \sqrt{1-\alpha_t}\bm{\epsilon_{t-1}}
    \label{eq:forward_randomvar}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bm{\epsilon_{t-1}} \sim \mathcal{N}(\bm{0}, \bm{I})\)</span> and
the earlier <span class="math notranslate nohighlight">\(\bm{x}_t\)</span> can be recursively inserted into the
formula. Recalling that the sum <span class="math notranslate nohighlight">\(Z = X + Y\)</span> of two normally
distributed random variables
<span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu_X, \sigma_Y^2)\)</span> and
<span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\mu_Y, \sigma_Y^2)\)</span> is again normally
distributed according to
<span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    x_t &amp; = \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1-\alpha_{t-1}}\bm{\epsilon}_{t-2} \right) + \sqrt{1-\alpha_{t}} \bm{\epsilon}_{t-1} \\
        &amp; = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1})} \bm{\epsilon}_{t-2} + \sqrt{1-\alpha_{t}} \bm{\epsilon}_{t-1}         \\
        &amp; = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1}) + (1-\alpha_{t})} \bm{\bar{\epsilon}}_{t-2}\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bm{\bar{\epsilon}}_{t-2}\)</span> is the sum of the random
variables up to <span class="math notranslate nohighlight">\(t-2\)</span> (again Gaussian). The second term can of
course be simplified to</p>
<div class="math notranslate nohighlight">
\[\bm{x}_t = \sqrt{\alpha_{t}\alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}} \bm{\bar{\epsilon}}_{t-2}\]</div>
<p>which is exactly the same form as in
Eq. <a class="reference external" href="#eq:forward_randomvar">[eq:forward_randomvar]</a>. Therefore the
final form is</p>
<div class="math notranslate nohighlight">
\[\bm{x}_t = \sqrt{\bar{\alpha}_{t}} \bm{x}_{t-2} + \sqrt{1-\bar{\alpha}_{t}} \bm{\bar{\epsilon}}_{t-2}\]</div>
<p>with <span class="math notranslate nohighlight">\(\bar{\alpha_t} = \prod_{s=1}^{t}\alpha_s\)</span> as before.</p>
</section>
</section>
<section id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h1>
<p>List the conclusions of your work and give evidence for these. Often,
the discussion and the conclusion sections are fused.</p>
</section>
<section id="discussion">
<h1>Discussion<a class="headerlink" href="#discussion" title="Link to this heading"></a></h1>
<p>The discussion section gives an interpretation of what you have done
<a href="#id1"><span class="problematic" id="id2">:raw-latex:`\cite{day2006wap}`</span></a>:</p>
<ul class="simple">
<li><p><em>What do your results mean?</em> Here you discuss, but you do not
recapitulate results. Describe principles, relationships and
generalizations shown. Also, mention inconsistencies or exceptions
you found.</p></li>
<li><p><em>How do your results relate to other’s work?</em> Show how your work
agrees or disagrees with other’s work. Here you can rely on the
information you presented in the “related work” section.</p></li>
<li><p><em>What are implications and applications of your work?</em> State how your
methods may be applied and what implications might be.</p></li>
</ul>
<p>Make sure that introduction/related work and the discussion section act
as a pair, i.e. “be sure the discussion section answers what the
introduction section asked” <a href="#id3"><span class="problematic" id="id4">:raw-latex:`\cite{day2006wap}`</span></a>.</p>
</section>
<section id="experiments-and-results">
<h1>Experiments and Results<a class="headerlink" href="#experiments-and-results" title="Link to this heading"></a></h1>
<p>Describe the evaluation you did in a way, such that an independent
researcher can repeat it. Cover the following questions:</p>
<ul class="simple">
<li><p><em>What is the experimental setup and methodology?</em> Describe the
setting of the experiments and give all the parameters in detail
which you have used. Give a detailed account of how the experiment
was conducted.</p></li>
<li><p><em>What are your results?</em> In this section, a <em>clear description</em> of
the results is given. If you produced lots of data, include only
representative data here and put all results into the appendix.</p></li>
</ul>
<section id="influence-of-schedules-and-image-size-on-the-forward-diffusion">
<span id="sec-forward-diff-experiments"></span><h2>Influence of Schedules and Image Size on the Forward Diffusion<a class="headerlink" href="#influence-of-schedules-and-image-size-on-the-forward-diffusion" title="Link to this heading"></a></h2>
<p>Ho et al. had derived a closed form solution to the forward process of
DDPMs and Nichol et al. investigated alternative options for the noise
scheduling. <a href="#id5"><span class="problematic" id="id6">:raw-latex:`\autocite{ho2020denoising,nichol2021improved}`</span></a>
They concluded that the important parameters to model are not the
variances <span class="math notranslate nohighlight">\(\beta\)</span> of the transitions, but the variances
<span class="math notranslate nohighlight">\(1-\bar{\alpha}\)</span> of the closed-form forward process, since they
are the ones responsible for the destruction of information.</p>
<p>They decided to go with a squared cosine function, since this would be
close to linear smooth out towards the critical beginning and end points
of the process. In Fig.<a class="reference external" href="#fig:alphadash">5.1</a> you can see how
<span class="math notranslate nohighlight">\(1-\bar{\alpha}\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> behave for both approaches. It
is immediately visible that the variances reach the maximum too early
and flatten out for the linear schedule. This leads to the intuition
that the last few steps are not very useful.</p>
<p>The intution can experimentally confirmed by measuring how closely we
get to isotropic noise when passing samples through the forward process.
For this a batch of 50 times the same image was passed through the
different steps of the process and the covariance matrix was calculated.
As a metric for how close the covariance matrix was to the identity
covariance matrix of pure i.i.d Gaussian noise, the identity matrix was
subtracted and the mean of the absolute value of the matrix calculated.
The results can be seen in Fig. <a class="reference external" href="#fig:noisecloseness">5.2</a> and
confirm the intuition: When using linear scheduling we reach the closest
point to pure noise already after around 600 steps for small images, and
after around 700 for larger images. Cosine scheduling also performs
worse on smaller images than on larger ones, but is still capable
providing value for at least 850 timesteps.</p>
</section>
</section>
<section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h1>
<p>Give an introduction to the topic you have worked on:</p>
<ul class="simple">
<li><p><em>What is the rationale for your work?</em> Give a sufficient description
of the problem, e.g. with a general description of the problem
setting, narrowing down to the particular problem you have been
working on in your thesis. Allow the reader to understand the problem
setting.</p></li>
<li><p><em>What is the scope of your work?</em> Given the above background, state
briefly the focus of the work, what and how you did.</p></li>
<li><p><em>How is your thesis organized?</em> It helps the reader to pick the
interesting points by providing a small text or graph which outlines
the organization of the thesis. The structure given in this document
shows how the general structuring shall look like. However, you may
fuse chapters or change their names according to the requirements of
your thesis.</p></li>
</ul>
<section id="focus-of-this-work">
<h2>Focus of this Work<a class="headerlink" href="#focus-of-this-work" title="Link to this heading"></a></h2>
</section>
<section id="thesis-organization">
<h2>Thesis Organization<a class="headerlink" href="#thesis-organization" title="Link to this heading"></a></h2>
</section>
</section>
<section id="materials-and-methods">
<h1>Materials and Methods<a class="headerlink" href="#materials-and-methods" title="Link to this heading"></a></h1>
<p>The objectives of the “Materials and Methods” section are the following:</p>
<ul class="simple">
<li><p><em>What are tools and methods you used?</em> Introduce the environment, in
which your work has taken place - this can be a software package, a
device or a system description. Make sure sufficiently detailed
descriptions of the algorithms and concepts (e.g. math) you used
shall be placed here.</p></li>
<li><p><em>What is your work?</em> Describe (perhaps in a separate chapter) the key
component of your work, e.g. an algorithm or software framework you
have developed.</p></li>
</ul>
</section>
<section id="generative-machine-learning">
<h1>Generative Machine Learning<a class="headerlink" href="#generative-machine-learning" title="Link to this heading"></a></h1>
<section id="bayesian-formulation-of-latent-variable-models">
<h2>Bayesian Formulation of Latent Variable Models<a class="headerlink" href="#bayesian-formulation-of-latent-variable-models" title="Link to this heading"></a></h2>
<p>Before getting started it is important to define the terms used in the
next sections, since they all stem from Bayesian statistics. The
Bayesian theorem can be written as</p>
<div class="math notranslate nohighlight">
\[p(z|x) = \frac{p(x|z)p(z)}{p(x)}\]</div>
<p>where it is implicitly assumed that <span class="math notranslate nohighlight">\(p\)</span> is a probability density
function over two continuous random variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(z\)</span>.
The formula holds in general, but in generative modeling and machine
learning it is usually assumed that the letter <span class="math notranslate nohighlight">\(z\)</span> represents a
random variable in latent space (unobserved) from which – after training
– can be sampled to generate new data, whereas <span class="math notranslate nohighlight">\(x\)</span> is the random
variable that represents the training samples (observed space).</p>
<p>Using above described ordering, the four terms in this formula use
distinct names:</p>
<dl class="simple">
<dt><span class="math notranslate nohighlight">\(p(x)\)</span></dt><dd><p>is called the <em>evidence</em> or the <em>marginal likelihood</em>. It encompasses
the actual observations of the data.</p>
</dd>
<dt><span class="math notranslate nohighlight">\(p(z)\)</span></dt><dd><p>is called the <em>prior</em>, since it exposes information on <span class="math notranslate nohighlight">\(z\)</span>
before any conditioning.</p>
</dd>
<dt><span class="math notranslate nohighlight">\(p(z|x)\)</span></dt><dd><p>is called the <em>posterior</em>. It describes the distribution over
<span class="math notranslate nohighlight">\(z\)</span> after (<em>post</em>) having seen the evidence <span class="math notranslate nohighlight">\(x\)</span>.</p>
</dd>
<dt><span class="math notranslate nohighlight">\(p(x|z)\)</span></dt><dd><p>is called the <em>likelihood</em>. It gives the literal likelihood of
observing an example <span class="math notranslate nohighlight">\(x\)</span> when choosing the latent space to be a
specific <span class="math notranslate nohighlight">\(z\)</span>.</p>
</dd>
</dl>
</section>
<section id="variational-autoencoders">
<h2>Variational Autoencoders<a class="headerlink" href="#variational-autoencoders" title="Link to this heading"></a></h2>
<p>One of the most straightforward examples of a generative model, where
the goal is to find such a latent space representation of the training
sample distribution, is the Variational Autoencoder
(VAE) <a href="#id7"><span class="problematic" id="id8">:raw-latex:`\autocite{kingma2022autoencoding}`</span></a>. The name of the
VAE stems from the Autoencoder, a network that tries to recreate its
output through a bottleneck and thereby learns a compressed
representation of the
data. <a href="#id9"><span class="problematic" id="id10">:raw-latex:`\autocite{https://doi.org/10.1002/aic.690370209}`</span></a>
Autoencoders bear similarity to other dimension reduction methods like
Principal Component Analysis (PCA) and therefore were first published
under the name <em>Nonlinear principal component analysis</em>. The
<em>variational</em> part in the VAE stems from the fact that it does not learn
to recreate input samples, but is rather optimized to represent the
distribution over the training samples as a combination of a
parameterized latent distribution <span class="math notranslate nohighlight">\(p_{\theta_z}(z)\)</span> and a neural
network mapping <span class="math notranslate nohighlight">\(p_{\theta_{NN}}(x|z)\)</span> between the latent space
and the sample space. The latent distribution is chosen such that
sampling from it is easy, which allows the neural network to create new
data samples (e.g. a multivariate Gaussian).</p>
<p>Marginalizing
<span class="math notranslate nohighlight">\(p_\theta(x) = \int p_{\theta_{NN}}(x|z) p_{\theta_z}(z) dz = \frac{p_{\theta_{NN_{out}}}(x|z) p_{\theta_z}(z)}{p_{\theta_{NN_{in}}}(z|x)}\)</span>
requires another approximation of the intractable posterior
<span class="math notranslate nohighlight">\(p_{\theta_{NN_{in}}}(z|x)\)</span> during training. A schematic of a VAE
is shown in Fig. <a class="reference external" href="#fig:vae">8.1</a>.</p>
<p>The neural networks usually do not contain stochastic layers, but are
deterministic mappings between latent space and sample space
<span class="math notranslate nohighlight">\(x \sim p_{\theta_{NN_{out}}}(x|z) \Rightarrow x = f_{\theta_{NN_{out}}}(z)\)</span>.
The hope is, that after training the encoder
<span class="math notranslate nohighlight">\(p_{\theta_{NN_{in}}}\)</span> can be removed and sampling from <span class="math notranslate nohighlight">\(z\)</span>
is the same as sampling from <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="diffusion-denoising-probabilistic-models">
<h2>Diffusion Denoising Probabilistic Models<a class="headerlink" href="#diffusion-denoising-probabilistic-models" title="Link to this heading"></a></h2>
<p>Diffusion Denoising Probabilistic Models (DDPMs or Diffusion Models) are
a generative model that learn the distribution of images in a training
set. During training, sample images are gradually destroyed by adding
noise over many iterations and a neural network is trained, such that
these steps can be inverted.</p>
<p>As the name suggests, image content is diffused in timesteps, therefore
we use the random variable <span class="math notranslate nohighlight">\(\bm{x}_0\)</span> to represent our original
training images, <span class="math notranslate nohighlight">\(\bm{x}_t\)</span> for (partially noisy) images at an
intermediate timestep and <span class="math notranslate nohighlight">\(\bm{x}_T\)</span> for images at the end of the
process where almost all information has been destroyed and the
distribution <span class="math notranslate nohighlight">\(q(\bm{x}_T)\)</span> largely follows an isotropic Gaussian
distribution – a Gaussian distribution with the identity matrix as
covariance matrix, but a non-zero means vector.</p>
<p>Once the network is trained to create a less noisy image
<span class="math notranslate nohighlight">\(\bm{x}_{t-1}\)</span> from <span class="math notranslate nohighlight">\(\bm{x}_t\)</span>, we should be able to sample
some new <span class="math notranslate nohighlight">\(\bm{x}_T\)</span> and generate new samples from the training
distribution <span class="math notranslate nohighlight">\(q(\bm{x}_0)\)</span> by passing it many times through the
network until all noise is removed.</p>
<section id="forward-diffusion-process">
<h3>Forward Diffusion Process<a class="headerlink" href="#forward-diffusion-process" title="Link to this heading"></a></h3>
<section id="mathematical-description">
<h4>Mathematical Description<a class="headerlink" href="#mathematical-description" title="Link to this heading"></a></h4>
<p>In order to derive a training objective it is important to understand
the workings of the <em>forward diffusion process</em>. During this process,
i.i.d (independent and identically distributed) Gaussian noise is
applied to the image over many discrete timesteps. A <em>variance schedule</em>
defines the means at variances (<span class="math notranslate nohighlight">\(\sqrt{1-\beta}\)</span> and
<span class="math notranslate nohighlight">\(\beta\)</span>) of the added noise at every
timestep. <a href="#id11"><span class="problematic" id="id12">:raw-latex:`\autocite{ho2020denoising}`</span></a> The whole process can
be expressed as a Markov chain (depicted in
Fig. <a class="reference external" href="#fig:forward_diffusion">8.2</a>)</p>
<div class="math notranslate nohighlight">
\[q(\bm{x}_T|\bm{x}_0) = q(\bm{x}_0) \prod_{t=1}^{T} q(\bm{x}_{t}|\bm{x}_{t-1})\]</div>
<p>with the transition distributions
<span class="math notranslate nohighlight">\(q(\bm{x}_t|\bm{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \bm{x}_{t-1}, \beta_t I)\)</span>.
An example of iterative destruction of an image by this process is shown
in Fig. <a class="reference external" href="#fig:forward_naoshima">8.3</a>.</p>
<p>Gladly it is not necessary to sample noise again and again in order to
arrive at <span class="math notranslate nohighlight">\(\bm{x}_t\)</span>, since Ho et al. derived a closed-form
solution to the sampling
procedure. <a href="#id13"><span class="problematic" id="id14">:raw-latex:`\autocite{ho2020denoising}`</span></a> For this, the
variance schedule is first reparameterized as <span class="math notranslate nohighlight">\(1-\beta = \alpha\)</span></p>
<div class="math notranslate nohighlight">
\[q(\bm{x}_t | \bm{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \bm{x}_{t-1}, (1-\alpha_t) \bm{I})
    \label{eq:forward_alpha}\]</div>
<p>and the closed-form solution for <span class="math notranslate nohighlight">\(q(\bm{x}_t|\bm{x}_0)\)</span> is derived
by introducing the cumulative product
<span class="math notranslate nohighlight">\(\bar{\alpha_t} = \prod_{s=1}^{t}\alpha_s\)</span> as</p>
<div class="math notranslate nohighlight">
\[q(\bm{x}_t|\bm{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha_t}}\bm{x}_0, (1-\bar{\alpha_t})\bm{I})
    \label{eq:forward_alphadash}\]</div>
<p>The derivation that leads from
Eq. <a class="reference external" href="#eq:forward_alpha">[eq:forward_alpha]</a> to
Eq. <a class="reference external" href="#eq:forward_alphadash">[eq:forward_alphadash]</a> will be left to
appendix <a class="reference external" href="#app:forward">2.1</a>.</p>
</section>
<section id="influence-of-scheduling-functions">
<h4>Influence of Scheduling Functions<a class="headerlink" href="#influence-of-scheduling-functions" title="Link to this heading"></a></h4>
<p>The process of information destruction is dependent on the chosen
variance schedule, the number of steps and the image size. Beyond the
most simple case – a constant variance over time – Ho et al. opted for
the second most simple option, a linear schedule, where the variance
<span class="math notranslate nohighlight">\(\beta_t\)</span> grows linearly in
<span class="math notranslate nohighlight">\(t\)</span>. <a href="#id15"><span class="problematic" id="id16">:raw-latex:`\autocite{ho2020denoising}`</span></a> Nichol et al. later
found that a cosine-based schedule gives better results, since it does
not destruct information quite as quickly, making it more informative in
the last few timesteps. <a href="#id17"><span class="problematic" id="id18">:raw-latex:`\autocite{nichol2021improved}`</span></a> Own
experiments exploring above mentioned parameters are explained
in <a class="reference external" href="#sec:forward_diff_experiments">5.1</a>.</p>
</section>
</section>
<section id="reverse-diffusion-process">
<h3>Reverse Diffusion Process<a class="headerlink" href="#reverse-diffusion-process" title="Link to this heading"></a></h3>
<p>DDPMs can be viewed as latent space models in a similar way that
Generative Adversarial Nets or Variational Autoencoders
can. <a href="#id19"><span class="problematic" id="id20">:raw-latex:`\autocite{goodfellow2014generative,kingma2022autoencoding}`</span></a></p>
<p>In DDPMs the reverse process is again a Markov chain and can therefore
again be factorized</p>
<div class="math notranslate nohighlight">
\[q(\bm{x}_0|\bm{x}_T) = q(\bm{x}_T) \prod_{t=T}^{1} q(\bm{x}_{t-1}|\bm{x}_{t})\]</div>
<p>which means that our network does not learn to approximate the full
inversion, but rather just the transition probabilities
<span class="math notranslate nohighlight">\(q(\bm{x}_{t-1}|\bm{x}_{t})\)</span> in the chain, which are transitions
between several intermediate latent distributions.</p>
</section>
</section>
<section id="latent-variable-models-compared">
<h2>Latent Variable Models Compared<a class="headerlink" href="#latent-variable-models-compared" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>VAE</p></th>
<th class="head"><p>GAN</p></th>
<th class="head"><p>DDPM</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>prior</p></td>
<td><p><span class="math notranslate nohighlight">\(p(z)\)</span>,
parameterized
of any shape</p></td>
<td><p><span class="math notranslate nohighlight">\(p(z)\)</span>,
parameterized
of any shape</p></td>
<td><p>:math
:<cite>p(bm{x}_t)</cite>,
same shape as
samples</p></td>
</tr>
<tr class="row-odd"><td><p>posterior</p></td>
<td><p><span class="math notranslate nohighlight">\(p(z|x)\)</span>,
modeled with
neural network</p></td>
<td><p><span class="math notranslate nohighlight">\(p(z|x)\)</span>,
modeled with
loss function</p></td>
<td><p>:ma
th:<cite>q(bm{x}_t|
bm{x}_{t-1})</cite>,
modeled as step
in forward
diffusion
process</p></td>
</tr>
<tr class="row-even"><td><p>likelihood</p></td>
<td><p><span class="math notranslate nohighlight">\(p(x
|z)=f_{NN}(z)\)</span>,
modeled with
neural network</p></td>
<td><p><span class="math notranslate nohighlight">\(p(x
|z)=f_{NN}(z)\)</span>,
modeled with
neural network</p></td>
<td><p><span class="math notranslate nohighlight">\(q(\bm{
x}_{t-1}|\bm{x}
_{t}) = \mathca
l{N}(k_1 f_{NN}
, k_2 f_{NN})\)</span>,
modeled with
Gaussian
sampling with
parameters
estimated by
neural network</p></td>
</tr>
</tbody>
</table>
</section>
<section id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Link to this heading"></a></h2>
<p>In generative machine learning we would want our model to learn the
distribution that generated out training examples. Often this
distribution is conditioned on some description (e.g. text) or on the
corruption process in our case, where we use generative models to solve
inverse problems. Assuming our original data distribution (of images) is
<span class="math notranslate nohighlight">\(p(x)\)</span>, then we try to find a parameterized variational machine
learning model (<span class="math notranslate nohighlight">\(q_{\theta}(x)\)</span>) that will closely match the data
distribution.</p>
<p>In order for this <span class="math notranslate nohighlight">\(q_{\theta}(x)\)</span> to be trained we need a
differentiable loss function that expresses “closeness” in a
distributional sense. The usual approach to this is to use the
Kullback-Leibler (KL) divergence.</p>
<section id="kullback-leibler-divergence">
<h3>Kullback-Leibler Divergence<a class="headerlink" href="#kullback-leibler-divergence" title="Link to this heading"></a></h3>
</section>
<section id="wasserstein-distance">
<h3>Wasserstein Distance<a class="headerlink" href="#wasserstein-distance" title="Link to this heading"></a></h3>
<p>A different approach to comparing the similarity of distributions is the
Wasserstein metric, successfully used in the Wasserstein
GAN. <a href="#id21"><span class="problematic" id="id22">:raw-latex:`\autocite{arjovsky2017wasserstein}`</span></a>.</p>
</section>
<section id="variational-lower-bound">
<h3>Variational Lower Bound<a class="headerlink" href="#variational-lower-bound" title="Link to this heading"></a></h3>
<p>As mentioned previously, the goal is to approximate a true data
distribution <span class="math notranslate nohighlight">\(p^*(x)\)</span> with a parameterized distribution
<span class="math notranslate nohighlight">\(p_\theta(x) = \int p_\theta(x|z)p(z)dz\)</span>, from which sampling is
easy, since the prior <span class="math notranslate nohighlight">\(p(z)\)</span> is very simple.</p>
</section>
</section>
</section>
<section id="related-work">
<h1>Related Work<a class="headerlink" href="#related-work" title="Link to this heading"></a></h1>
<p>Describe the other’s work in the field, with the following purposes in
mind:</p>
<ul class="simple">
<li><p><em>Is the overview concise?</em> Give an overview of the most relevant work
to the needed extent. Make sure the reader can understand your work
without referring to other literature.</p></li>
<li><p><em>Does the compilation of work help to define the “niche” you are
working in?</em> Another purpose of this section is to lay the groundwork
for showing that you did significant work. The selection and
presentation of the related work should enable you to name the
implications, differences and similarities sufficiently in the
“discussion” section.</p></li>
</ul>
<p><strong>My first and last thesis</strong>
<strong>Subtitle Subtitle Subtitle</strong>
Master’s Thesis
Eckhart Immerheiser
Department of …</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><strong>A</strong>dvisors:</p></td>
<td><p>Egon Hasenfratz-Schreier</p></td>
</tr>
<tr class="row-even"><td><p><strong>S</strong>upervisor:</p></td>
<td><p>Prof. Dr. Luc van Gool</p></td>
</tr>
</tbody>
</table>
<p>January 10, 2020</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../_autosummary/diffusion_models.utils.mp_setup.DDP_Proc_Group.html" class="btn btn-neutral float-left" title="diffusion_models.utils.mp_setup.DDP_Proc_Group" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../idea_corner.html" class="btn btn-neutral float-right" title="Idea Corner" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Lionel Peer.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>